\documentclass[11pt]{article}

% Standard packages available in Overleaf
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{microtype}

% Make citations work without .bib file (they'll show as [citation-key])
% Remove this and uncomment bibliography section below when ready to add references.bib
\providecommand{\cite}[1]{[#1]}

% Title and Author Information
\title{SimRAG Reproduction: A Simplified Implementation of\\Retrieval-Augmented Generation with Fine-Tuning}

% Anonymous submission - remove author information
% \author{Anonymous Author(s)}
% \affiliation{Anonymous Institution(s)}

\begin{document}

\maketitle

\begin{abstract}
% TODO: Write abstract (150-250 words)
% Should summarize:
% - What paper/method you reproduced
% - Key hypothesis you tested
% - Your simplified approach
% - Main findings
% - Contribution/insights

We present a simplified reproduction of SimRAG, implementing core RAG components (semantic retrieval, Qdrant/ChromaDB vector storage, two-stage QLoRA fine-tuning) to verify that RAG + fine-tuning improves domain-specific QA performance. Using consumer hardware (RTX 3080/10GB VRAM) with the QLoRA-optimized Qwen 2.5 1.5B-Instruct model (7B supported but not tested), we demonstrate feasibility and validate SimRAG's core insights while providing an accessible framework for understanding RAG systems. The implementation supports multiple AI providers (Claude, Purdue GenAI API, HuggingFace) for synthetic QA generation and uses HuggingFace Transformers with 4-bit quantization for efficient inference.

\end{abstract}

\section{Introduction}
\label{sec:introduction}

% TODO: Expand introduction (0.5-1 page)
% Should include:
% - Motivation: Why RAG and SimRAG are important
% - Paper selection: Why you chose SimRAG
% - Problem statement: What you're trying to verify
% - Scope: What aspects you're focusing on (simplification)

\subsection{Motivation}

\textbf{LLM Limitations}: Hallucination and knowledge cutoff limit LLM effectiveness on domain-specific tasks. RAG addresses this by combining retrieval with generation, grounding responses in external knowledge.

\textbf{Domain-Specific Challenge}: Standard RAG struggles with specialized corpora requiring domain adaptation. SimRAG proposes self-improving fine-tuning for RAG tasks.

\textbf{Reproduction Goal}: Verify SimRAG's claim that RAG + fine-tuning improves domain-specific QA. Demonstrate feasibility on constrained hardware (RTX 3080/CPU) for educational accessibility.

\subsection{Paper Selection and Hypothesis}

\textbf{Selection Rationale}: Clear testable hypothesis, accessible implementation (standard RAG components), educational value (self-improvement via synthetic data).

\textbf{Core Hypothesis}: \textit{Two-stage fine-tuning (instruction-following + domain adaptation) improves RAG performance on domain-specific documents vs. vanilla RAG}.

\textbf{Success Criteria}: (1) +2–3\% improvement in average context relevance scores, (2) better answer quality on domain questions, (3) feasible on consumer hardware (RTX 3080/CPU).

\subsection{Scope and Simplifications}

\textbf{Implemented}: (1) RAG system (sentence-transformers + Qdrant/ChromaDB), (2) Stage 1 fine-tuning (Alpaca dataset), (3) Stage 2 domain adaptation (synthetic QA with self-improvement loop), (4) Baseline vanilla RAG, (5) Multi-provider AI gateway (Claude/Purdue/HuggingFace), (6) Docker support for reproducibility.

\textbf{Simplifications}: (1) Smaller model (Qwen 2.5 1.5B-Instruct; 7B supported but not trained/tested), (2) QLoRA fine-tuning (4-bit quantization + LoRA adapters), (3) Smaller corpus (5k–20k chunks), (4) Limited self-improvement rounds. \textbf{Justification}: Verify improvement \textit{trend} rather than match exact numbers; demonstrate hardware accessibility with memory-efficient training.

\section{Related Work}
\label{sec:related}

\textbf{RAG Foundations}: Lewis et al.~\cite{lewis2020retrieval} introduced RAG combining parametric (model weights) and non-parametric (retrieved docs) knowledge. Subsequent work improved retrieval~\cite{karpukhin2020dense}, reranking~\cite{wang2022retrieval}, and hybrid approaches~\cite{ram2023retrieval}.

\textbf{Domain Adaptation}: RAFT~\cite{zhang2024raft} adapts models for domain-specific retrievers. Other methods use contrastive learning~\cite{izacard2022atlas} or instruction tuning~\cite{wei2022finetuned}, but require labeled domain data.

\textbf{SimRAG}: Self-improving framework generating synthetic QA pairs from unlabeled documents. Two-stage fine-tuning: (1) general instruction-following, (2) domain adaptation with synthetic QA. Reduces need for labeled data.

\textbf{Our Contribution}: Simplified reproduction verifying SimRAG on constrained hardware (consumer-grade vs. large-scale infrastructure).

\section{Method}
\label{sec:method}

% TODO: Expand method section (1-1.5 pages)
% This is the core of your reproduction study

\subsection{Hypothesis Distillation}

\textbf{Original Claim}: Self-improving RAG achieves improvements via iterative fine-tuning on synthetic data.

\textbf{Testable Hypothesis}: \textit{Two-stage fine-tuning (instruction-following + domain adaptation) improves RAG on domain documents vs. vanilla RAG}.

\textbf{Success Criteria}: (1) +2–3\% avg context relevance (cosine similarity), (2) better answer quality (coherence, domain terminology), (3) feasible on RTX 3080/CPU.

\textbf{Focus}: Verify improvement \textit{direction}, not exact numbers (hardware/implementation differ from original).

\subsection{System Architecture}

\textbf{Three Subsystems}: (1) \textit{RAG}: VectorStore (Qdrant for training, ChromaDB for testing), DocumentRetriever (embeddings), AIGateway (Claude/Purdue/HuggingFace), orchestrated by BasicRAG. (2) \textit{Fine-tuning}: InstructionFollowing (Stage 1), SyntheticQAGeneration (QA pairs), DomainAdaptation (Stage 2), all inherit SimRAGBase. QLoRA training (4-bit quantization + LoRA adapters) with HuggingFace Transformers for inference. (3) \textit{Experiment Management}: Model registry (versioning, metadata, experiment run IDs with round information), configuration system (hardware-aware, env var overrides).

\subsubsection{Retrieval-Augmented Generation}

\textbf{Components}: (1) \textit{Document ingestion}: HTML/Markdown/text → chunks (200–500 tokens), preserves boundaries. (2) \textit{Embeddings}: sentence-transformers → dense vectors → Qdrant (training, in-memory by default) or ChromaDB (local testing). (3) \textit{Retrieval}: Query embedding → top-$k=5$ search → filter by threshold (0.7) → return docs + scores. (4) \textit{Generation}: Prompt = "Context: [docs]\n\nQuestion: [query]\n\nAnswer:" → LLM (HuggingFace Transformers with 4-bit quantization for baseline and fine-tuned models).

\subsubsection{Model Fine-Tuning}

\textbf{Base Models}: Primary model: Qwen 2.5 1.5B-Instruct (trained and tested). Qwen 2.5 7B-Instruct is supported by the framework but not trained or tested in this reproduction. HuggingFace Transformers, causal LM objective. QLoRA training: 4-bit quantization (NF4), LoRA adapters (rank=16, alpha=32), target modules (q\_proj, k\_proj, v\_proj, o\_proj).

\textbf{Stage 1}: Alpaca dataset (52K examples), format "Question: [instruction]\nAnswer: [response]". LR=$5 \times 10^{-5}$, batch=8 (GPU), epochs=3. QLoRA adapters saved (~100MB for 1.5B model; 7B would require ~400MB but was not trained).

\textbf{Stage 2}: Continues from Stage 1 adapters. Synthetic QA from domain docs (Claude/Purdue API/HuggingFace for questions, RAG for answers). Filter: context score $\geq 0.7$. Same hyperparams, fewer epochs (1–2). Supports self-improvement loop (multiple rounds, each using improved model). Model notes include round information (e.g., "Round 3/5").

\textbf{Optimizations}: QLoRA (4-bit + LoRA) enables training on 10GB VRAM, HuggingFace Transformers with 4-bit quantization for efficient inference, gradient accumulation (effective batch=16), mixed precision (FP16), model versioning with round tracking. Docker support for reproducible environments.

\subsection{Baseline Implementation}

\textbf{Vanilla RAG}: Same components as SimRAG (ChromaDB for testing, sentence-transformers, top-$k=5$, threshold=0.7) but uses base HuggingFace model (Qwen 2.5 1.5B) with 4-bit quantization, without fine-tuning.

\textbf{Justification}: Isolates fine-tuning effect; identical infrastructure → differences attributed to fine-tuning. Represents standard RAG practice.

\textbf{Scope}: Tests RAG + fine-tuning combination; no separate "fine-tuned without RAG" baseline needed.

\section{Experiments}
\label{sec:experiments}

% TODO: Expand experiments section (1-1.5 pages)
% This should detail your experimental design

\subsection{Experimental Design}

\textbf{Dataset}: HTML docs (Docker/DevOps/cloud computing), 5k–20k chunks, domain-specific knowledge required.

\textbf{Test Questions}: 30 questions covering Docker, CI/CD, DevOps, Google Cloud Platform, and Python (e.g., "What is Docker?", "How does CI/CD work?", "What are Docker layers and how do they optimize image builds?"), require retrieval + generation with domain terminology.

\textbf{Metrics}: (1) \textit{Primary}: Avg context relevance (cosine similarity query↔docs). (2) \textit{Secondary}: Response time, answer length, qualitative assessment (relevance, terminology, coherence).

\textbf{Hardware}: Primary experiments: Qwen 2.5 1.5B-Instruct, RTX 3080 (10GB VRAM), QLoRA training (~3–4GB VRAM), batch=8. The framework supports Qwen 2.5 7B-Instruct (would require ~8–10GB VRAM) but this model was not trained or tested.

\subsection{Implementation Details}

\textbf{Software}: Python 3.12+, PyTorch 2.5+ (CUDA), Transformers 4.30+, sentence-transformers 2.2+, Qdrant 1.7+ (training), ChromaDB (testing), datasets library, PEFT (LoRA), bitsandbytes (4-bit quantization), anthropic (optional, for Claude API). Docker support for reproducibility. Modular package structure.

\textbf{Models}: Primary model: Qwen 2.5 1.5B-Instruct (trained and tested). Qwen 2.5 7B-Instruct is supported but not used. QLoRA: 4-bit NF4 quantization, LoRA rank=16, alpha=32, dropout=0.05. Training: FP16 mixed precision, batch=8, gradient accumulation=2 (effective batch=16), LR=$5 \times 10^{-5}$, max length=512.

\textbf{Hyperparameters}: Stage 1: Alpaca (52K examples), causal LM loss, 3 epochs. Stage 2: Synthetic QA (2 per doc, score≥0.7), 1–2 epochs. Both: AdamW, linear LR decay, gradient accumulation, mixed precision (FP16), no gradient checkpointing (to avoid hangs).

\textbf{Retrieval}: Top-$k=5$, threshold=0.7, sentence-transformers (384-dim), Qdrant (training, in-memory by default) or ChromaDB (local testing, in-memory).

\subsection{Evaluation Methodology}

\textbf{Quantitative}: Primary = avg context relevance (mean cosine similarity query↔all retrieved docs). Also: response time, answer length, success rate.

\textbf{Comparison}: Baseline vs SimRAG on same questions/corpus. Record: (1) context scores, (2) answers, (3) response time. Improvement = $\frac{\text{SimRAG}_{\text{avg}} - \text{Baseline}_{\text{avg}}}{\text{Baseline}_{\text{avg}}} \times 100\%$.

\textbf{Qualitative}: Manual inspection: relevance, domain terminology, context grounding, coherence.

\textbf{Limitations}: Small corpus (5k–20k), limited test set (30 questions), no human evaluation, hardware constraints. Sufficient for hypothesis verification.

\section{Results and Analysis}
\label{sec:results}

% TODO: Present and analyze results (1-1.5 pages)
% This is where you show what you found

\subsection{Retrieval Performance}
% TODO: Present retrieval results
% - Context similarity scores
% - Top-k retrieval effectiveness
% - Document relevance analysis
% - Figures/tables showing retrieval metrics

\subsection{Generation Quality}
% TODO: Analyze answer quality
% - Comparison: baseline vs fine-tuned
% - Response time analysis
% - Domain-specific improvements
% - Examples of good/poor retrievals

\subsection{Resource Analysis}
% TODO: Computational considerations
% - Training time comparisons (laptop vs PC)
% - Memory requirements
% - Model size trade-offs
% - Scalability observations

\subsection{Discussion}
% TODO: Interpret results
% - Do results support the hypothesis?
% - What worked well? What didn't?
% - Limitations and challenges encountered
% - Insights gained from reproduction

\section{Conclusion}
\label{sec:conclusion}

% TODO: Write conclusion (0.5 page)
% Should summarize:
% - Key findings
% - Verification of core hypothesis
% - Contributions of the reproduction
% - Future work or extensions
% - Lessons learned

\section*{Acknowledgments}
% TODO: Optional acknowledgments
% Keep anonymous if needed

% Bibliography
% Citations currently show as [citation-key] using a temporary definition above
% To add proper bibliography:
% 1. Create references.bib file in Overleaf
% 2. Remove the \providecommand{\cite} line near the top
% 3. Uncomment the lines below:
% \bibliography{references}
% \bibliographystyle{plain}

% Appendices (if needed, don't count toward page limit)
\begin{appendix}
\section{Additional Results}
% TODO: Additional figures, tables, or detailed results

\section{Implementation Details}
% TODO: Code snippets, configuration examples, etc.

\end{appendix}

\end{document}

