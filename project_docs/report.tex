\documentclass[11pt]{article}

% Standard packages available in Overleaf
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{microtype}

% Bibliography is now included inline below
% For final submission, consider creating a references.bib file and using \bibliography{references}

% Title and Author Information
\title{SimRAG Reproduction: A Simplified Implementation of\\Retrieval-Augmented Generation with Fine-Tuning}

% Anonymous submission - remove author information
% \author{Anonymous Author(s)}
% \affiliation{Anonymous Institution(s)}

\begin{document}

\maketitle

\begin{abstract}
% TODO: Write abstract (150-250 words)
% Should summarize:
% - What paper/method you reproduced
% - Key hypothesis you tested
% - Your simplified approach
% - Main findings
% - Contribution/insights

We present a simplified reproduction of SimRAG, implementing core RAG components (semantic retrieval, Qdrant/ChromaDB vector storage, two-stage QLoRA fine-tuning) to verify that RAG + fine-tuning improves domain-specific QA performance. Using consumer hardware (RTX 3080/10GB VRAM) with the QLoRA-optimized Qwen 2.5 1.5B-Instruct model (7B supported but not tested), we demonstrate feasibility and validate SimRAG's core insights while providing an accessible framework for understanding RAG systems. The implementation supports multiple AI providers (Claude, Purdue GenAI API, HuggingFace) for synthetic QA generation and uses HuggingFace Transformers with 4-bit quantization for efficient inference.

\end{abstract}

\section{Introduction}
\label{sec:introduction}

% TODO: Expand introduction (0.5-1 page)
% Should include:
% - Motivation: Why RAG and SimRAG are important
% - Paper selection: Why you chose SimRAG
% - Problem statement: What you're trying to verify
% - Scope: What aspects you're focusing on (simplification)

\subsection{Motivation}

\textbf{LLM Limitations}: Hallucination and knowledge cutoff limit LLM effectiveness on domain-specific tasks. RAG addresses this by combining retrieval with generation, grounding responses in external knowledge.

\textbf{Domain-Specific Challenge}: Standard RAG struggles with specialized corpora requiring domain adaptation. SimRAG proposes self-improving fine-tuning for RAG tasks.

\textbf{Reproduction Goal}: Verify SimRAG's claim that RAG + fine-tuning improves domain-specific QA. Demonstrate feasibility on constrained hardware (RTX 3080/CPU) for educational accessibility.

\subsection{Paper Selection and Hypothesis}

\textbf{Selection Rationale}: Clear testable hypothesis, accessible implementation (standard RAG components), educational value (self-improvement via synthetic data).

\textbf{Core Hypothesis}: \textit{Two-stage fine-tuning (instruction-following + domain adaptation) improves RAG performance on domain-specific documents vs. vanilla RAG}.

\textbf{Success Criteria}: (1) +2–3\% improvement in average context relevance scores, (2) better answer quality on domain questions, (3) feasible on consumer hardware (RTX 3080/CPU).

\subsection{Scope and Simplifications}

\textbf{Implemented}: (1) RAG system (sentence-transformers + Qdrant/ChromaDB), (2) Stage 1 fine-tuning (Alpaca dataset), (3) Stage 2 domain adaptation (synthetic QA with self-improvement loop), (4) Baseline vanilla RAG, (5) Multi-provider AI gateway (Claude/Purdue/HuggingFace), (6) Docker support for reproducibility.

\textbf{Simplifications}: (1) Smaller model (Qwen 2.5 1.5B-Instruct; 7B supported but not trained/tested), (2) QLoRA fine-tuning (4-bit quantization + LoRA adapters), (3) Smaller corpus (5k–20k chunks), (4) Limited self-improvement rounds. \textbf{Justification}: Verify improvement \textit{trend} rather than match exact numbers; demonstrate hardware accessibility with memory-efficient training.

\section{Related Work}
\label{sec:related}

\textbf{RAG Foundations}: RAG (Retrieval-Augmented Generation) combines parametric knowledge (model weights) with non-parametric knowledge (retrieved documents) to ground language model responses in external knowledge sources. Various approaches have been developed to improve retrieval quality, reranking, and hybrid retrieval strategies.

\textbf{Domain Adaptation}: Domain-specific RAG adaptation methods aim to improve performance on specialized corpora. These approaches often require labeled domain data or use techniques like contrastive learning or instruction tuning to adapt models to specific domains.

\textbf{SimRAG}: Self-improving framework generating synthetic QA pairs from unlabeled documents~\cite{cheng2025simrag}. Two-stage fine-tuning: (1) general instruction-following, (2) domain adaptation with synthetic QA. Reduces need for labeled data.

\textbf{Our Contribution}: Simplified reproduction verifying SimRAG on constrained hardware (consumer-grade vs. large-scale infrastructure).

\section{Method}
\label{sec:method}

% TODO: Expand method section (1-1.5 pages)
% This is the core of your reproduction study

\subsection{Hypothesis Distillation}

\textbf{Original Claim}: Self-improving RAG achieves improvements via iterative fine-tuning on synthetic data.

\textbf{Testable Hypothesis}: \textit{Two-stage fine-tuning (instruction-following + domain adaptation) improves RAG on domain documents vs. vanilla RAG}.

\textbf{Success Criteria}: (1) +2–3\% avg context relevance (cosine similarity), (2) better answer quality (coherence, domain terminology), (3) feasible on RTX 3080/CPU.

\textbf{Focus}: Verify improvement \textit{direction}, not exact numbers (hardware/implementation differ from original).

\subsection{System Architecture}

% TODO: Expand with architecture diagram/description
\textbf{Three Subsystems}:
\begin{itemize}
    \item \textit{RAG}: VectorStore (Qdrant/ChromaDB), DocumentRetriever, AIGateway (Claude/Purdue/HuggingFace), BasicRAG orchestrator
    \item \textit{Fine-tuning}: InstructionFollowing (Stage 1), SyntheticQAGeneration, DomainAdaptation (Stage 2), all inherit SimRAGBase. QLoRA training.
    \item \textit{Experiment Management}: Model registry, configuration system
\end{itemize}

\subsubsection{Retrieval-Augmented Generation}

% TODO: Expand with detailed component descriptions
\textbf{Components}:
\begin{itemize}
    \item Document ingestion: HTML/Markdown/text → chunks (200–500 tokens)
    \item Embeddings: sentence-transformers → Qdrant (training) or ChromaDB (testing)
    \item Retrieval: top-$k=5$, threshold=0.7
    \item Generation: Prompt format "Context: [docs]\n\nQuestion: [query]\n\nAnswer:" → HuggingFace Transformers (4-bit quantization)
\end{itemize}

\subsubsection{Model Fine-Tuning}

% TODO: Expand with training details, hyperparameters
\textbf{Base Models}: Qwen 2.5 1.5B-Instruct (trained/tested). 7B supported but not used. QLoRA~\cite{dettmers2023qlora}: 4-bit NF4, LoRA rank=16, alpha=32.

\textbf{Stage 1}: Alpaca (52K examples), LR=$5 \times 10^{-5}$, batch=8, epochs=3. Adapters ~100MB.

\textbf{Stage 2}: Synthetic QA from domain docs, context score $\geq 0.7$, 1–2 epochs. Self-improvement loop with round tracking.

\textbf{Optimizations}: QLoRA enables 10GB VRAM training, gradient accumulation (effective batch=16), FP16, Docker support.

\subsection{Baseline Implementation}

% TODO: Expand baseline description
\textbf{Vanilla RAG}: Same components as SimRAG but uses base HuggingFace model (Qwen 2.5 1.5B, 4-bit quantization) without fine-tuning.

\textbf{Justification}: Isolates fine-tuning effect; identical infrastructure.

% TODO: Add scope discussion

\section{Experiments}
\label{sec:experiments}

% TODO: Expand experiments section (1-1.5 pages)
% This should detail your experimental design

\subsection{Experimental Design}

\textbf{Dataset}: HTML docs (Docker/DevOps/cloud computing), 5k–20k chunks, domain-specific knowledge required.

\textbf{Test Questions}: 30 questions covering Docker, CI/CD, DevOps, Google Cloud Platform, and Python (e.g., "What is Docker?", "How does CI/CD work?", "What are Docker layers and how do they optimize image builds?"), require retrieval + generation with domain terminology.

\textbf{Metrics}: (1) \textit{Primary}: Avg context relevance (cosine similarity query↔docs). (2) \textit{Secondary}: Response time, answer length, qualitative assessment (relevance, terminology, coherence).

\textbf{Hardware}: Primary experiments: Qwen 2.5 1.5B-Instruct, RTX 3080 (10GB VRAM), QLoRA training (~3–4GB VRAM), batch=8. The framework supports Qwen 2.5 7B-Instruct (would require ~8–10GB VRAM) but this model was not trained or tested.

\subsection{Implementation Details}

\textbf{Software}: Python 3.12+, PyTorch 2.5+ (CUDA), Transformers 4.30+, sentence-transformers 2.2+, Qdrant 1.7+ (training), ChromaDB (testing), datasets library, PEFT (LoRA), bitsandbytes (4-bit quantization), anthropic (optional, for Claude API). Docker support for reproducibility. Modular package structure.

\textbf{Models}: Primary model: Qwen 2.5 1.5B-Instruct (trained and tested). Qwen 2.5 7B-Instruct is supported but not used. QLoRA: 4-bit NF4 quantization, LoRA rank=16, alpha=32, dropout=0.05. Training: FP16 mixed precision, batch=8, gradient accumulation=2 (effective batch=16), LR=$5 \times 10^{-5}$, max length=512.

\textbf{Hyperparameters}: Stage 1: Alpaca (52K examples), causal LM loss, 3 epochs. Stage 2: Synthetic QA (2 per doc, score≥0.7), 1–2 epochs. Both: AdamW, linear LR decay, gradient accumulation, mixed precision (FP16), no gradient checkpointing (to avoid hangs).

\textbf{Retrieval}: Top-$k=5$, threshold=0.7, sentence-transformers (384-dim), Qdrant (training, in-memory by default) or ChromaDB (local testing, in-memory).

\subsection{Evaluation Methodology}

\textbf{Quantitative}: Primary = avg context relevance (mean cosine similarity query↔all retrieved docs). Also: response time, answer length, success rate.

\textbf{Comparison}: Baseline vs SimRAG on same questions/corpus. Record: (1) context scores, (2) answers, (3) response time. Improvement = $\frac{\text{SimRAG}_{\text{avg}} - \text{Baseline}_{\text{avg}}}{\text{Baseline}_{\text{avg}}} \times 100\%$.

\textbf{Qualitative}: Manual inspection: relevance, domain terminology, context grounding, coherence.

\textbf{Limitations}: Small corpus (5k–20k), limited test set (30 questions), no human evaluation, hardware constraints. Sufficient for hypothesis verification.

\section{Results and Analysis}
\label{sec:results}

% TODO: Present and analyze results (1-1.5 pages)
% This is where you show what you found

\subsection{Retrieval Performance}

% TODO: Expand with detailed analysis
\textbf{Key Results}:
\begin{itemize}
    \item Stage 2 models (v4.2, v4.9): $\mu = 0.316$ (95\% CI: [0.291, 0.340], $n=150$)
    \item Stage 1 model (v1.8): $\mu = 0.321$ (95\% CI: [0.273, 0.369], $n=50$)
    \item Identical scores for baseline and fine-tuned (expected - same retrieval pipeline)
    \item Statistical analysis: overlapping CIs, no significant difference
\end{itemize}

% TODO: Add interpretation, figures/tables

\subsection{Generation Quality}

% TODO: Expand with detailed analysis and examples
\textbf{Answer Quality}:
\begin{itemize}
    \item Stage 2: 0.769 (v4.2), 0.764 (v4.9) vs baseline 0.801 (decrease 3.2\%, 3.7\%)
    \item Stage 1: 0.772 vs baseline 0.795 (decrease 2.2\%)
    \item Metrics: length, context relevance, question relevance, refusal detection
\end{itemize}

\textbf{Response Time}:
\begin{itemize}
    \item Stage 2: 53.4s (v4.2), 58.4s (v4.9) vs baseline 41.2s (+29.5\%, +41.7\%)
    \item Stage 1: 46.2s vs baseline 34.3s (+34.6\%)
\end{itemize}

% TODO: Add qualitative examples, analysis of why quality decreased

\subsection{Resource Analysis}

% TODO: Expand with detailed resource usage, training times
\textbf{Training}:
\begin{itemize}
    \item VRAM: $\sim$3--4GB for 1.5B model (RTX 3080, 10GB total)
    \item Stage 1: Alpaca (52K examples), 3 epochs, batch=8, effective batch=16
    \item Stage 2: Synthetic QA, 1--2 epochs, similar resources
\end{itemize}

\textbf{Model Size}: LoRA adapters $\sim$100MB (99.3\% reduction from $\sim$3GB full model)

\textbf{Inference}: 30--42\% slower, $\sim$2GB VRAM (both baseline and fine-tuned use 4-bit quantization)

% TODO: Add training time comparisons, scalability analysis

\subsection{Discussion}

% TODO: Expand interpretation of results
\textbf{Hypothesis Verification}: Results do not support hypothesis. Context scores identical (expected), quality decreased 2--4\%, response time increased 30--42\%. No statistical significance.

\textbf{Possible Explanations}:
\begin{itemize}
    \item Small model capacity (1.5B may be insufficient)
    \item Training data quality (synthetic QA filtering threshold $\geq 0.7$)
    \item Metric limitations (rule-based word overlap)
    \item Hyperparameter tuning needed
\end{itemize}

\textbf{What Worked}: QLoRA on consumer hardware, experimental design, reproducible framework, model versioning

\textbf{Challenges}: No improvement from fine-tuning, response time overhead, metric limitations

% TODO: Add insights, lessons learned, what this means for the field

\section{Conclusion}
\label{sec:conclusion}

% TODO: Write conclusion (0.5 page)
\textbf{Key Findings}: 
\begin{itemize}
    \item Successfully implemented SimRAG pipeline on consumer hardware (RTX 3080) using QLoRA
    \item Results do not support hypothesis: identical context scores, quality decreased 2--4\%, response time increased 30--42\%
    \item No statistical significance (overlapping confidence intervals)
\end{itemize}

\textbf{Hypothesis Verification}: Results do not confirm SimRAG's claim. Possible reasons: model capacity, hyperparameters, metrics, overfitting.

\textbf{Contributions}: Feasibility on consumer hardware, experimental design validation, reproducible framework

\textbf{Ongoing Work}: New model training with improved Stage 1 settings to test if better initial training enables Stage 2 improvements.

\textbf{Future Work}: Larger models (7B), better QA generation, semantic metrics, LoRA tuning, domain-specific analysis

% TODO: Add lessons learned, implications for the field

\section*{Acknowledgments}
% TODO: Optional acknowledgments
% Keep anonymous if needed

% Bibliography
% Note: For final submission, create a references.bib file and uncomment the lines below
% For now, citations are shown inline with full references

\begin{thebibliography}{2}

% Papers from project_docs that were actually used
\bibitem{cheng2025simrag}
Cheng, X., Zhang, Y., Li, H., \& Sun, M. (2025). SimRAG: Self-improving retrieval-augmented generation. In \textit{Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)}. Association for Computational Linguistics.

\bibitem{dettmers2023qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., \& Zettlemoyer, L. (2023). QLoRA: Efficient finetuning of quantized LLMs. \textit{Advances in Neural Information Processing Systems (NeurIPS 2023)}.

\end{thebibliography}

% Appendices (if needed, don't count toward page limit)
\begin{appendix}
\section{Additional Results}
% TODO: Additional figures, tables, or detailed results

\section{Implementation Details}
% TODO: Code snippets, configuration examples, etc.

\end{appendix}

\end{document}

