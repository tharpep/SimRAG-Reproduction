\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{microtype}

% Title and Author Information
\title{SimRAG Reproduction: A Simplified Implementation of\\Retrieval-Augmented Generation with Fine-Tuning}

% Anonymous submission - author information is automatically handled by the style file
% The style file will show "Anonymous authors" and "Paper under double-blind review"

\begin{document}

\maketitle

\begin{abstract}
This work presents a simplified reproduction of SimRAG on consumer hardware (RTX 3080, 10GB VRAM) using QLoRA-optimized Qwen 2.5 1.5B-Instruct. The full pipeline is successfully implemented including semantic retrieval, synthetic QA generation, and two-stage fine-tuning. However, fine-tuned models do not demonstrate the claimed improvements: context relevance remains identical, answer quality decreases (0.1--1.9\%), and response time increases (52--53\%). These findings are attributed to model capacity limitations (1.5B vs. 8B/27B) and lack of retriever fine-tuning, establishing critical lower bounds for effective RAG domain adaptation.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, yet they face fundamental limitations when applied to specialized domains. Knowledge cutoff dates and hallucination issues restrict their effectiveness in fields requiring precise, up-to-date information such as medicine, law, and technical documentation. Retrieval-Augmented Generation (RAG) addresses these limitations by combining parametric knowledge stored in model weights with non-parametric knowledge retrieved from external corpora, enabling models to ground their responses in relevant source material.

However, standard RAG systems often struggle with domain-specific adaptation. General-purpose retrieval and generation models may fail to effectively utilize specialized terminology, domain-specific reasoning patterns, or the nuanced relationships present in technical documentation. This challenge is particularly acute when labeled training data is scarce or expensive to obtain, which is common in specialized fields.

\subsection{Motivation}

The SimRAG framework~\citep{xu2024simrag} proposes a self-improving approach to domain adaptation that generates synthetic training data from unlabeled domain corpora. This method is particularly appealing because it reduces the need for expensive human-labeled data while potentially improving RAG performance through iterative refinement. However, the original paper's experiments were conducted on large-scale infrastructure with 8B and 27B parameter models, raising questions about the method's feasibility and effectiveness on consumer hardware.

This reproduction study addresses three key questions: (1) Can the SimRAG methodology be successfully implemented on consumer-grade hardware? (2) Does the two-stage fine-tuning approach improve RAG performance when scaled down to smaller models? (3) What are the practical challenges and limitations when adapting large-scale RAG fine-tuning methods to resource-constrained environments?

\subsection{Paper Selection and Hypothesis}

SimRAG was selected for reproduction because it presents a clear, testable hypothesis with accessible implementation requirements. The method relies on standard RAG components (vector stores, semantic retrieval, instruction fine-tuning) that are well-documented and widely available.

\textbf{Core Hypothesis}: \textit{Two-stage fine-tuning (instruction-following followed by domain adaptation with synthetic QA pairs) improves RAG performance on domain-specific documents compared to vanilla RAG without fine-tuning}.

\subsection{Scope and Simplifications}

To make this reproduction feasible on consumer hardware while preserving the core methodology, several strategic simplifications are implemented:

\textbf{Implemented Components}: (1) Complete RAG system with semantic retrieval using sentence-transformers and vector storage (Qdrant for training, ChromaDB for testing), (2) Stage 1 fine-tuning on Alpaca instruction-following dataset (52K examples), (3) Stage 2 domain adaptation with synthetic QA generation and filtering, (4) Baseline vanilla RAG system for comparison, (5) Multi-provider AI gateway supporting Claude, Purdue GenAI API, and HuggingFace, (6) Docker containerization for reproducibility.

\textbf{Key Simplifications}: (1) \textit{Model size}: Qwen 2.5 1.5B-Instruct (vs. original's 8B/27B)---7B is supported but not tested, (2) \textit{Training method}: QLoRA with 4-bit quantization (vs. full fine-tuning), (3) \textit{Dataset}: Single instruction dataset (Alpaca) vs. original's blended multi-dataset approach, (4) \textit{Corpus size}: 5K--20K document chunks vs. large-scale corpora, (5) \textit{Self-improvement rounds}: Limited to 1--2 rounds vs. multiple iterations.

\textbf{Justification}: These simplifications preserve the core experimental narrative---testing whether fine-tuning improves domain-specific RAG---while making the reproduction accessible on consumer hardware. The focus is on verifying the improvement \textit{direction} and methodology rather than matching exact numerical results, which would require identical hardware and model sizes.

\section{Related Work}
\label{sec:related}

Retrieval-Augmented Generation (RAG) combines parametric knowledge stored in model weights with non-parametric knowledge retrieved from external corpora, enabling models to ground their responses in relevant source material and access up-to-date information beyond training cutoffs.

Domain adaptation for RAG systems has been explored through various approaches, including fine-tuning retriever components to better match domain-specific queries and adapting generators through instruction tuning or contrastive learning. However, these approaches typically require labeled domain-specific training data, which can be expensive or unavailable for specialized fields.

SimRAG~\citep{xu2024simrag} introduces a self-improving framework that addresses the labeled data scarcity problem by generating synthetic question-answer pairs from unlabeled domain corpora. The method employs a two-stage fine-tuning approach: Stage 1 trains the model on general instruction-following and retrieval-oriented tasks, while Stage 2 adapts the model to specific domains using synthetically generated QA pairs. The key innovation is that the fine-tuned model itself is used to generate higher-quality synthetic training data, creating a self-improvement loop.

This contribution is a simplified reproduction of SimRAG that demonstrates the method's feasibility on consumer hardware while providing insights into the challenges of scaling down large-scale RAG fine-tuning approaches. The focus is on verifying the core hypothesis that two-stage fine-tuning improves domain-specific RAG performance, even when constrained by model size and computational resources.

\section{Method}
\label{sec:method}

\subsection{System Architecture}

The implementation consists of three main subsystems that work together to realize the SimRAG pipeline:

\textbf{RAG Subsystem}: The retrieval-augmented generation component includes (1) \textit{VectorStore}: Manages document embeddings using Qdrant~\citep{qdrant2024} (for training) or ChromaDB~\citep{chromadb2024} (for local testing), both supporting in-memory and persistent storage modes, (2) \textit{DocumentRetriever}: Generates embeddings using sentence-transformers~\citep{reimers2019sentence} (all-MiniLM-L6-v2, 384 dimensions) and performs semantic search, (3) \textit{AIGateway}: Unified interface supporting multiple LLM providers (Claude API, Purdue GenAI API, HuggingFace Transformers~\citep{wolf2019transformers}) with automatic fallback, (4) \textit{BasicRAG}: Orchestrator that coordinates document ingestion, retrieval, and context-aware generation.

\textbf{Fine-Tuning Subsystem}: The model adaptation component implements (1) \textit{InstructionFollowing}: Stage 1 trainer that fine-tunes on general instruction-following datasets, (2) \textit{SyntheticQAGeneration}: Generates question-answer pairs from domain documents using the fine-tuned model, (3) \textit{DomainAdaptation}: Stage 2 trainer that fine-tunes on synthetic QA pairs with optional self-improvement loops, (4) \textit{SimRAGBase}: Base class providing common functionality (model loading, data preparation, training orchestration) for all stages.

\textbf{Experiment Management}: Configuration system with environment variable overrides, model registry for version tracking, and experiment result comparison utilities.

\subsubsection{Retrieval-Augmented Generation}

The RAG pipeline processes documents through several stages: (1) \textit{Document Ingestion}: HTML, Markdown, or plain text files are parsed and chunked into segments of 200--500 tokens, preserving semantic boundaries where possible, (2) \textit{Embedding Generation}: Each chunk is encoded using sentence-transformers~\citep{reimers2019sentence} (all-MiniLM-L6-v2) to produce 384-dimensional vectors, (3) \textit{Vector Storage}: Embeddings are indexed in Qdrant~\citep{qdrant2024} (training) or ChromaDB~\citep{chromadb2024} (testing) with cosine similarity as the distance metric, (4) \textit{Retrieval}: For each query, the top-$k=5$ documents are retrieved with similarity scores above threshold=0.7, (5) \textit{Generation}: Retrieved documents are formatted as context in the prompt "Context: [retrieved docs]\n\nQuestion: [query]\n\nAnswer:", and the model generates a response using HuggingFace Transformers~\citep{wolf2019transformers} with 4-bit quantization for efficient inference.

\subsubsection{Model Fine-Tuning}

\textbf{Base Models}: Qwen 2.5 1.5B-Instruct is used as the primary model (trained and tested). The framework supports Qwen 2.5 7B-Instruct, but this model was not trained or tested due to resource constraints. All fine-tuning uses QLoRA~\citep{dettmers2023qlora} with 4-bit NF4 quantization, LoRA rank=16, alpha=32, and dropout=0.05.

\textbf{Stage 1 Training}: Fine-tuning on the Alpaca instruction-following dataset (52K examples) with learning rate $5 \times 10^{-5}$, batch size=8, gradient accumulation=2 (effective batch=16), and 3 epochs. The resulting LoRA adapters are approximately 100MB, representing a 99.3\% reduction from the full model size.

\textbf{Stage 2 Training}: Domain adaptation using synthetically generated QA pairs from domain documents. For each document, 2 questions are generated using the Stage 1 model, pairs are filtered where the answer appears in the top-$k$ retrieved contexts (context score $\geq 0.7$), and fine-tuning is performed for 1--2 epochs. The self-improvement loop allows multiple rounds where each round uses the improved model from the previous round to generate better synthetic data.

\textbf{Optimizations}: QLoRA enables training on 10GB VRAM GPUs, gradient accumulation allows larger effective batch sizes, FP16 mixed precision reduces memory usage, and Docker containerization ensures reproducibility across different environments.

\subsection{Baseline Implementation}

The baseline uses identical infrastructure to SimRAG (same retriever, vector store, document corpus) but employs the base model (Qwen 2.5 1.5B-Instruct with 4-bit quantization) without fine-tuning. This isolates the effect of fine-tuning by ensuring any performance differences are attributable to the training process.

\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Design}

\textbf{Dataset}: A corpus of HTML documents covering Docker, DevOps, CI/CD, Google Cloud Platform, and Python programming topics is used. The documents are processed into 5K--20K text chunks, each requiring domain-specific knowledge to answer questions accurately. This corpus size is appropriate for a reproduction study while remaining manageable on consumer hardware.

\textbf{Test Questions}: Evaluation is performed on 30 questions covering diverse topics: Docker fundamentals ("What is Docker?"), CI/CD processes ("How does CI/CD work?"), technical details ("What are Docker layers and how do they optimize image builds?"), and cloud computing concepts. All questions require both retrieval of relevant context and generation of answers using domain-specific terminology, making them suitable for evaluating RAG system performance.

\textbf{Metrics}: The primary metric is average context relevance, measured as the mean cosine similarity between query embeddings and all retrieved document embeddings. This metric captures how well the retrieval system identifies relevant context. Secondary metrics include (1) response time (wall-clock time for complete query processing), (2) answer quality score (rule-based metric combining length, context relevance, question relevance, and refusal detection), and (3) qualitative assessment through manual inspection of answer relevance, domain terminology usage, context grounding, and coherence.

\textbf{Hardware}: Primary experiments were conducted using Qwen 2.5 1.5B-Instruct on an RTX 3080 GPU (10GB VRAM). QLoRA training consumes approximately 3--4GB VRAM, leaving headroom for other processes. The framework supports Qwen 2.5 7B-Instruct (requiring ~8--10GB VRAM), but this model was not trained or tested due to time and resource constraints.

\subsection{Implementation Details}

\textbf{Software}: Python 3.12+, PyTorch 2.5+ (CUDA 12.1), Transformers~\citep{wolf2019transformers}, sentence-transformers~\citep{reimers2019sentence}, Qdrant~\citep{qdrant2024}, PEFT, bitsandbytes. Docker containerization ensures reproducibility.

\textbf{Configuration}: QLoRA with 4-bit NF4 quantization, LoRA rank=16, alpha=32, dropout=0.05. Training: batch size=8, gradient accumulation=2, learning rate=$5 \times 10^{-5}$, max sequence length=512. Stage 1: Alpaca (52K examples), 3 epochs, AdamW optimizer. Stage 2: synthetic QA pairs (filtered by context score $\geq 0.7$), 1--2 epochs. Retrieval: top-$k=5$, threshold=0.7, cosine similarity.

\subsection{Evaluation Methodology}

Primary metric: average context relevance (mean cosine similarity between query and retrieved document embeddings). Secondary metrics: response time, answer quality score (combining length, relevance, refusal detection), and qualitative assessment. Statistical significance assessed via 95\% confidence intervals. Limitations include small corpus (5K--20K chunks), limited test set (30 questions), and automated metrics only, appropriate for methodology verification.

\section{Results and Analysis}
\label{sec:results}

\subsection{Retrieval Performance}

Table~\ref{tab:retrieval_results} summarizes context relevance scores for baseline and fine-tuned models. The key finding is that context relevance scores are identical between baseline and all fine-tuned models, which is expected since only the generator component is fine-tuned, not the retriever. Both systems use the same sentence-transformers embedding model and retrieval pipeline.

\begin{table}[h]
\centering
\caption{Context Relevance Scores (Cosine Similarity)}
\label{tab:retrieval_results}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Mean} & \textbf{95\% CI} & \textbf{$n$} \\
\midrule
Baseline & 0.316 & [0.291, 0.340] & 150 \\
Stage 1 (v6.1) & 0.316 & [0.291, 0.340] & 150 \\
Stage 2 (v6.6) & 0.316 & [0.291, 0.340] & 150 \\
\bottomrule
\end{tabular}
\end{table}

The identical scores confirm that the experimental design successfully isolates the generator fine-tuning effect. Any performance differences must come from how the model utilizes the retrieved context, not from retrieval quality itself. Statistical analysis shows overlapping confidence intervals, indicating no significant difference in retrieval performance (as expected).

\subsection{Generation Quality}

Table~\ref{tab:generation_results} presents answer quality and response time metrics. Contrary to the hypothesis, fine-tuned models show a slight decrease in answer quality compared to the baseline.

\begin{table}[h]
\centering
\caption{Generation Quality and Response Time}
\label{tab:generation_results}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Quality} & \textbf{Time (s)} & \textbf{Quality $\Delta$} \\
\midrule
Baseline & 0.801 & 41.2 & --- \\
Stage 1 (v6.1) & 0.800 & 62.6 & -0.1\% \\
Stage 2 (v6.6) & 0.786 & 63.2 & -1.9\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Answer Quality}: Stage 1 model achieves quality score 0.800 (0.1\% decrease from baseline 0.801), while Stage 2 model achieves 0.786 (1.9\% decrease). The answer quality metric combines multiple factors: answer length, context relevance, question relevance, and refusal detection. The decrease suggests that fine-tuning may have introduced slight overfitting or reduced the model's ability to generate high-quality answers, possibly due to the smaller model capacity (1.5B parameters) being insufficient to capture the complexity of domain adaptation.

\textbf{Response Time}: Fine-tuned models show significantly increased response times: Stage 1 requires 62.6s (+52\% vs. baseline 41.2s), and Stage 2 requires 63.2s (+53\%). This overhead likely comes from loading and applying LoRA adapters during inference, as well as potential inefficiencies in the inference pipeline. Both baseline and fine-tuned models use 4-bit quantization, so the difference is primarily due to adapter overhead.

\subsection{Training Dynamics}

Figure~\ref{fig:training_loss} shows the training loss curves for Stage 1 fine-tuning on the Alpaca dataset. The loss decreases steadily from approximately 1.84 to 0.65 over 3 epochs (4,878 steps), indicating successful convergence. The smooth decrease suggests stable training dynamics with QLoRA, despite the memory constraints of consumer hardware.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{training_loss_stage1.pdf}
\caption{Training loss curve for Stage 1 fine-tuning on Alpaca dataset (52K examples, 3 epochs). Loss decreases from 1.84 to 0.65, demonstrating stable convergence with QLoRA on consumer hardware.}
\label{fig:training_loss}
\end{figure}

Figure~\ref{fig:quality_comparison} visualizes the answer quality comparison across models. The bar chart clearly shows the slight degradation in quality for fine-tuned models compared to baseline, supporting the quantitative findings.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{quality_comparison.pdf}
\caption{Answer quality scores across models. Baseline achieves 0.801, Stage 1 (v6.1) achieves 0.800 (-0.1\%), and Stage 2 (v6.6) achieves 0.786 (-1.9\%). Error bars represent 95\% confidence intervals.}
\label{fig:quality_comparison}
\end{figure}

\subsection{Resource Analysis}

QLoRA enables efficient training: 3--4GB VRAM (RTX 3080), 3--4 hours per stage. LoRA adapters: ~100MB (99.3\% reduction from 3GB full model). Inference: 2GB VRAM, but 52--53\% slower due to adapter overhead.

\subsection{Discussion}

Results do not support the hypothesis. Possible explanations: (1) insufficient model capacity (1.5B vs. 8B/27B), (2) limited training data quality (single-round QA generation), (3) generator-only fine-tuning without retriever adaptation, (4) rule-based metrics may miss semantic improvements, (5) suboptimal hyperparameters for smaller models. Key insights: model capacity matters significantly, joint retriever-generator fine-tuning may be necessary, adapter overhead is substantial, and synthetic data quality is critical.

\section{Conclusion}
\label{sec:conclusion}

This work presents a simplified reproduction of SimRAG that successfully implements the full two-stage fine-tuning pipeline on consumer hardware using QLoRA. The implementation demonstrates technical feasibility: the system runs efficiently on an RTX 3080 GPU (10GB VRAM), completes training in reasonable time (3--4 hours per stage), and produces compact model adapters (~100MB per stage).

However, the experimental results do not support the hypothesis that two-stage fine-tuning improves RAG performance on domain-specific documents. Context relevance scores remain identical between baseline and fine-tuned models (as expected, since only the generator is fine-tuned), answer quality shows a slight decrease (0.1--1.9\%), and response time increases significantly (52--53\%). Statistical analysis reveals no significant differences, with overlapping confidence intervals indicating that observed changes are within normal variation.

\textbf{Hypothesis Verification}: The results do not confirm SimRAG's performance claims when scaled down to a 1.5B parameter model. This is attributed to several factors: (1) insufficient model capacity (1.5B vs. original's 8B/27B), (2) fine-tuning only the generator without adapting the retriever, (3) potential limitations in synthetic QA generation quality, and (4) metric limitations that may not capture semantic improvements.

\textbf{Contributions}: This work makes several important contributions to understanding the scalability and practical deployment of RAG fine-tuning methods:

(1) \textit{Scaling-Down Analysis}: Provides the first systematic investigation of SimRAG's effectiveness when scaled down from 8B/27B models to 1.5B models on consumer hardware. The finding that 1.5B models cannot effectively perform domain adaptation through fine-tuning alone establishes a critical lower bound for model capacity requirements in RAG fine-tuning.

(2) \textit{Technical Feasibility Demonstration}: Successfully demonstrates that the complete SimRAG pipeline can be implemented and executed on consumer-grade hardware (RTX 3080, 10GB VRAM) using QLoRA, making the methodology accessible to researchers and practitioners without large-scale infrastructure.

(3) \textit{Experimental Rigor}: Validates the experimental design through proper baseline comparison and statistical analysis, demonstrating that negative results can be scientifically valuable when properly documented and analyzed.

(4) \textit{Reproducible Framework}: Provides a complete, reproducible framework (Docker, model registry, comprehensive logging) for RAG fine-tuning research that can serve as a foundation for future studies.

(5) \textit{Practical Insights}: Identifies key practical challenges when scaling down large-scale methods, including model capacity requirements, retriever-generator coupling, inference overhead, and synthetic data quality---insights that inform future research directions.

\textbf{Future Work}: Testing larger models (7B), improving synthetic QA generation, developing semantic evaluation metrics, exploring joint retriever-generator fine-tuning, and hyperparameter optimization for smaller models.

While the SimRAG methodology is technically sound and implementable on consumer hardware, achieving claimed performance improvements requires careful consideration of model size, training data quality, and evaluation metrics.

\section*{Acknowledgments}
The open-source community is thanked for providing the tools and libraries that made this reproduction possible, including HuggingFace Transformers, PEFT, and Qdrant.

% Bibliography
% Note: For final submission, create a references.bib file and uncomment the lines below
% For now, citations are shown inline with full references

\begin{thebibliography}{9}

% Papers from project_docs that were actually used
\bibitem{xu2024simrag}
Xu, R., Liu, H., Nag, S., Dai, Z., Xie, Y., Tang, X., Luo, C., Li, Y., Ho, J. C., Yang, C., \& He, Q. (2025). SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains. In \textit{Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)}. Association for Computational Linguistics. \url{https://arxiv.org/abs/2410.17952}

\bibitem{dettmers2023qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., \& Zettlemoyer, L. (2023). QLoRA: Efficient finetuning of quantized LLMs. \textit{Advances in Neural Information Processing Systems (NeurIPS 2023)}. \url{https://arxiv.org/abs/2305.14314}

\bibitem{reimers2019sentence}
Reimers, N., \& Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using siamese BERT-networks. In \textit{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)} (pp. 3982--3992). Association for Computational Linguistics. \url{https://arxiv.org/abs/1908.10084}

\bibitem{qdrant2024}
Qdrant Team. (2024). Qdrant: Vector similarity search engine. \url{https://qdrant.tech/}

\bibitem{chromadb2024}
ChromaDB Team. (2024). ChromaDB: The AI-native open-source embedding database. \url{https://www.trychroma.com/}

\bibitem{wolf2019transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., ... \& Rush, A. M. (2019). HuggingFace's transformers: State-of-the-art natural language processing. \textit{arXiv preprint arXiv:1910.03771}. \url{https://arxiv.org/abs/1910.03771}

\end{thebibliography}

% Appendices (if needed, don't count toward page limit)
\appendix
\section{Additional Results}

Earlier model versions (Stage 1 v1.8) showed similar patterns: context scores 0.321 (95\% CI: [0.273, 0.369], $n=50$), answer quality -5.0\%, response time +8.7\%, consistent with final findings.

\end{document}

