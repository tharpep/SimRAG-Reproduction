---
title: "Checkpoint 1 – Preliminary Code & Results"
format: revealjs
---

## 1) Problem Statement & Goal {.smaller}

**Problem:** AI assistants struggle with domain-specific knowledge from specialized sources like company documents or class notes, performing well on general web data but failing on targeted corpora.

**Goal:** Reproduce SimRAG's self-improving RAG approach on constrained hardware (single RTX 3080) to demonstrate democratization of domain-specific RAG research for students and small labs.

**Hypothesis:** SimRAG's two-stage approach (retrieval-oriented fine-tuning + domain-adaptive fine-tuning with synthetic QA generation) can achieve measurable improvements over vanilla RAG even with hardware constraints.

---

## 2) Methodology Overview {.smaller}

**Technical Approach:** Implement SimRAG's self-training methodology with:
- **Stage I:** Fine-tune LLM on instruction-following, QA, and search data
- **Stage II:** Generate synthetic domain QA pairs from unlabeled corpora, then fine-tune on these examples
- **Baseline:** Vanilla RAG with dense retriever + 7-8B LLM
- **Target:** 5k-20k document corpus with +2-3% Recall@k or EM/F1 gains

**Implementation Strategy:** Modular RAG system with separate tuning pipeline, using QLoRA for efficient fine-tuning on single GPU.

---

## 3) Code Snippet 1: RAG System Orchestration {.smaller}

```python
class BasicRAG:
    def __init__(self, collection_name=None, use_persistent=None):
        self.config = get_rag_config()
        self.collection_name = collection_name or self.config.collection_name
        self.gateway = AIGateway()
        self.vector_store = VectorStore(use_persistent=use_persistent)
        self.retriever = DocumentRetriever()
        self._setup_collection()
    
    def query(self, question, context_limit=None):
        context_limit = context_limit or self.config.top_k
        retrieved_docs = self.search(question, limit=context_limit)
        if not retrieved_docs:
            return "No relevant documents found.", [], []
        rag_context = "\n\n".join([doc for doc, _ in retrieved_docs])
        prompt = f"Context:\n{rag_context}\n\nQuestion: {question}\n\nAnswer:"
        
        answer = self.gateway.chat(prompt)
        return answer, context_docs, context_scores
```

---

## 4) Explanation of Snippet 1 {.smaller}

**Function:** Core RAG orchestration that coordinates vector storage, retrieval, and generation components.

**Key Components:**
- `AIGateway`: Abstracts different AI providers (Ollama, Purdue API)
- `VectorStore`: Manages Qdrant vector database operations
- `DocumentRetriever`: Handles embedding generation with sentence transformers

**Importance:** This is the foundation for SimRAG implementation - the vanilla RAG baseline that will be enhanced with self-training. The modular design allows easy integration of fine-tuned models in Stage II.

---

## 5) Code Snippet 2: Model Tuning with Versioning {.smaller}

```python
class BasicTuner:
    def __init__(self, model_name: str = "qwen3:1.7b", device: str = "auto", config=None):
        self.model_name = model_name
        self.device = self._get_device(device)
        self.config = config
        self.tokenizer = self.model = self.trainer = None
        self.registry = get_model_registry(self.config) if self.config else None
    
    def train(self, notes: Optional[str] = None):
        if not self.trainer:
            raise ValueError("Trainer not setup. Call setup_trainer() first.")
        self.trainer.train()
        if self.registry and self.config:
            new_version = self.registry.create_new_version(
                model_name=self.model_name,
                training_epochs=self.config.optimized_num_epochs,
                batch_size=self.config.optimized_batch_size,
                notes=notes
            )
            self.registry.register_version(new_version)
```

---

## 6) Explanation of Snippet 2 {.smaller}

**Function:** Model tuning framework with versioning and registry management for tracking SimRAG training phases.

**Key Features:**
- Model registry for version tracking and metadata management
- Hardware-optimized training parameters (laptop vs PC configurations)
- Support for Llama3.2-1B and Qwen2.5 models with automatic device detection
- Training time and loss tracking for performance analysis

**Importance:** This provides the infrastructure for SimRAG's two-stage approach. The versioning system will track Stage I (retrieval-oriented fine-tuning) and Stage II (domain-adaptive fine-tuning) models separately, enabling comparison and rollback capabilities.

---

## 7) Preliminary Result: RAG System Performance {.smaller}

**Status:** ✅ **Functional RAG Pipeline**

- **Vector Store:** Qdrant persistent storage with 8 document collections
- **Embeddings:** Sentence transformers (all-MiniLM-L6-v2) 384-dim vectors
- **AI Gateway:** Multi-provider support (Ollama llama3.2:1b, Purdue API)
- **Document Processing:** Docker, DevOps, Python, and algorithm docs

**Performance Metrics:**
- **Response Time:** 8.16s - 27.14s per query (llama3.2:1b model)
- **Document Retrieval:** 5 relevant documents per query
- **Similarity Scores:** 0.750-0.759 cosine similarity
- **Query Sessions:** 20+ successful sessions across 4 domains

**Example Q&A:**
- **Query:** "How does binary search work?" (11.58s)
- **Answer:** "Binary search works by repeatedly dividing the search interval in half and checking if the target value is less than or greater than the middle element. 1. Compare the target value with the middle element of the array. 2. If the target value matches the middle element, return the index where it was found. 3. Otherwise, eliminate half of the current interval based on whether the target value is less than or greater than the middle element. This process continues until the target value is found in the array or the interval is empty."
- **Context:** 5 documents with 0.750 similarity scores from binary search documentation

**Baseline:** Vanilla RAG system with measurable performance metrics.

---

## 8) Result Analysis & Next Steps {.smaller}

**Analysis:** The RAG foundation is solid with working document processing and query answering. The tuning framework is implemented but needs SimRAG-specific enhancements. Current response times (8.16-27.14s) and similarity scores (0.750-0.759) provide a baseline for measuring SimRAG improvements.

**Immediate Next Steps for SimRAG Integration:**

1. **Stage I Implementation:** Fine-tune on instruction-following + QA datasets

2. **Synthetic QA Generation:** Implement domain-specific question generation from document corpus

3. **Stage II Fine-tuning:** Train on synthetic QA pairs with quality filtering

4. **Performance Evaluation:** Measure Recall@k, EM/F1 improvements over baseline

5. **Hardware Optimization:** Implement QLoRA for efficient single-GPU training
