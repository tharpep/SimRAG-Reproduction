# Baseline RAG Configuration
model:
  retriever:
    name: "sentence-transformers/all-MiniLM-L6-v2"
    max_length: 512
    device: "cuda"
  
  generator:
    name: "microsoft/DialoGPT-medium"  # 7B model for constrained hardware
    max_length: 512
    temperature: 0.7
    do_sample: true
    device: "cuda"

data:
  corpus_size: 10000  # 5k-20k chunks as specified
  chunk_size: 256
  chunk_overlap: 50
  max_retrieved_docs: 5

evaluation:
  metrics: ["em", "f1", "recall_at_k", "ndcg"]
  k_values: [1, 5, 10]
  test_size: 0.2
  random_seed: 42

training:
  batch_size: 8
  learning_rate: 2e-5
  num_epochs: 3
  warmup_steps: 100

hardware:
  max_memory_gb: 12  # RTX 3080 constraint
  mixed_precision: true
  gradient_checkpointing: true
