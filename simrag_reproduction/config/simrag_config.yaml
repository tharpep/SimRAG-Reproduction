# SimRAG Configuration
model:
  retriever:
    name: "sentence-transformers/all-MiniLM-L6-v2"
    max_length: 512
    device: "cuda"
  
  generator:
    name: "microsoft/DialoGPT-medium"
    max_length: 512
    temperature: 0.7
    do_sample: true
    device: "cuda"
  
  # QLoRA parameters for efficient fine-tuning
  quantization:
    load_in_4bit: true
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: "nf4"

data:
  corpus_size: 10000
  chunk_size: 256
  chunk_overlap: 50
  max_retrieved_docs: 5
  
  # SimRAG specific
  synthetic_data:
    num_synthetic_pairs: 1000
    generation_temperature: 0.8
    diversity_threshold: 0.7

self_improvement:
  num_iterations: 3
  improvement_threshold: 0.02  # 2% improvement threshold
  max_synthetic_data_ratio: 0.3  # Max 30% synthetic data
  
  # Fine-tuning parameters
  fine_tune_retriever: true
  fine_tune_generator: true
  retriever_lr: 1e-5
  generator_lr: 2e-5

evaluation:
  metrics: ["em", "f1", "recall_at_k", "ndcg"]
  k_values: [1, 5, 10]
  test_size: 0.2
  random_seed: 42

training:
  batch_size: 4  # Smaller batch for memory efficiency
  learning_rate: 2e-5
  num_epochs: 2  # Fewer epochs for efficiency
  warmup_steps: 50
  gradient_accumulation_steps: 2

hardware:
  max_memory_gb: 12
  mixed_precision: true
  gradient_checkpointing: true
  use_qlora: true
