{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimRAG Model Testing on Google Colab\n",
    "\n",
    "This notebook tests **any fine-tuned SimRAG model** (Stage 1, Stage 2, or any checkpoint) using Google Colab's free T4 GPU (15GB VRAM).\n",
    "\n",
    "## ðŸ“‹ Complete Workflow\n",
    "\n",
    "**Step 1: Prepare Files Locally**\n",
    "```bash\n",
    "# Export your trained model\n",
    "simrag experiment export\n",
    "\n",
    "# Create documents ZIP\n",
    "cd data\n",
    "zip -r documents.zip documents/  # Linux/Mac\n",
    "# OR\n",
    "Compress-Archive -Path documents -DestinationPath documents.zip  # Windows\n",
    "```\n",
    "\n",
    "**Step 2: Run This Notebook**\n",
    "1. Upload `documents.zip` (when prompted)\n",
    "2. Notebook automatically runs baseline tests (base model, no fine-tuning)\n",
    "3. Upload your model ZIP (e.g., `checkpoint-1000-fixed.zip`)\n",
    "4. Notebook automatically runs fine-tuned model tests\n",
    "5. Automatic comparison and download\n",
    "\n",
    "**Step 3: View Results Locally**\n",
    "1. Download the `comparison_results_*.json` file from Colab\n",
    "2. Place it in: `comparison_results/` (project root)\n",
    "3. Run: `simrag experiment results` to view formatted results\n",
    "\n",
    "## âœ… Fair Comparison Guarantees\n",
    "\n",
    "Both baseline and fine-tuned models are tested with **identical conditions**:\n",
    "- **Same embedding model**: `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions)\n",
    "- **Same retrieval**: top_k=5 documents per question\n",
    "- **Same prompt format**: `Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:`\n",
    "- **Same test questions**: 30 standard questions from `get_test_questions()`\n",
    "- **Same evaluation metrics**: `evaluate_answer_quality()` (length, context relevance, question relevance, refusal detection)\n",
    "- **Same generation params**: temperature=0.7, max_tokens=512, top_p=0.9\n",
    "- **Same documents**: Identical document set indexed in ChromaDB\n",
    "\n",
    "## ðŸ“Š Evaluation Metrics\n",
    "\n",
    "**Context Score**: Average similarity between retrieved documents and query (0.0-1.0)\n",
    "- Higher = better document retrieval\n",
    "\n",
    "**Answer Quality Score**: Multi-metric evaluation (0.0-1.0)\n",
    "- **Length**: Reasonable answer length (20-500 chars = 1.0)\n",
    "- **Context Relevance**: Word overlap between answer and retrieved context\n",
    "- **Question Relevance**: Word overlap between answer and question\n",
    "- **Not Refusal**: Answer attempts to respond (not \"I don't know\")\n",
    "- **Overall**: Average of all quality metrics\n",
    "\n",
    "**Statistical Analysis**: \n",
    "- Mean Â± Standard Deviation\n",
    "- 95% Confidence Intervals\n",
    "- Statistical significance test (CI overlap)\n",
    "\n",
    "## ðŸŽ¯ What Models Can You Test?\n",
    "\n",
    "- âœ… **Any Stage 1 model** (instruction following)\n",
    "- âœ… **Any Stage 2 model** (domain adaptation)\n",
    "- âœ… **Any checkpoint** (checkpoint-500, checkpoint-1000, etc.)\n",
    "- âœ… **Any model version** (v1.0, v1.8, etc.)\n",
    "\n",
    "The notebook automatically detects model metadata from `adapter_config.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch peft accelerate bitsandbytes chromadb sentence-transformers beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "# Check GPU\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions (from simrag_reproduction/experiments/utils.py)\n",
    "\n",
    "These match your local testing infrastructure exactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLTextExtractor(HTMLParser):\n",
    "    \"\"\"Extract text content from HTML files (exact copy from utils.py)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text = []\n",
    "        self.skip_tags = {'script', 'style', 'meta', 'link', 'head'}\n",
    "        self.in_skip_tag = False\n",
    "    \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag.lower() in self.skip_tags:\n",
    "            self.in_skip_tag = True\n",
    "    \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag.lower() in self.skip_tags:\n",
    "            self.in_skip_tag = False\n",
    "        elif tag.lower() in {'p', 'div', 'br', 'li'}:\n",
    "            self.text.append('\\n')\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if not self.in_skip_tag:\n",
    "            cleaned = data.strip()\n",
    "            if cleaned:\n",
    "                self.text.append(cleaned)\n",
    "    \n",
    "    def get_text(self):\n",
    "        text = ' '.join(self.text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "def extract_text_from_html(html_path):\n",
    "    \"\"\"Extract text from HTML file\"\"\"\n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            html_content = f.read()\n",
    "        parser = HTMLTextExtractor()\n",
    "        parser.feed(html_content)\n",
    "        return parser.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {html_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def load_documents_from_folder(folder_path, include_html=True):\n",
    "    \"\"\"Load documents from folder (exact copy from utils.py)\"\"\"\n",
    "    folder = Path(folder_path).resolve()\n",
    "    if not folder.exists():\n",
    "        raise ValueError(f\"Folder not found: {folder_path}\")\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    # Load .txt and .md files\n",
    "    for ext in ['.txt', '.md']:\n",
    "        for file_path in folder.glob(f\"**/*{ext}\"):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read().strip()\n",
    "                    if content:\n",
    "                        documents.append(content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    # Load HTML files\n",
    "    if include_html:\n",
    "        for html_path in folder.glob(\"**/*.html\"):\n",
    "            text = extract_text_from_html(html_path)\n",
    "            if text:\n",
    "                documents.append(text)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def get_test_questions():\n",
    "    \"\"\"Standard test questions (exact copy from utils.py) - Expanded to 30 questions\"\"\"\n",
    "    return [\n",
    "        # Original questions (1-10)\n",
    "        \"What is Docker?\",\n",
    "        \"How does CI/CD work?\",\n",
    "        \"What is DevOps?\",\n",
    "        \"How do you containerize an application?\",\n",
    "        \"What is the difference between Docker and virtual machines?\",\n",
    "        \"How does Google Cloud Platform work?\",\n",
    "        \"What are the benefits of using containers?\",\n",
    "        \"Explain the Python standard library.\",\n",
    "        \"What is RAG in generative AI?\",\n",
    "        \"How do you build a Docker image?\",\n",
    "        \n",
    "        # Docker - Technical depth (11-18)\n",
    "        \"What are Docker layers and how do they optimize image builds?\",\n",
    "        \"How do Docker volumes work and when should you use them?\",\n",
    "        \"What is the difference between Docker COPY and ADD commands?\",\n",
    "        \"How does Docker networking work between containers?\",\n",
    "        \"What are Docker multi-stage builds and why are they useful?\",\n",
    "        \"How do you optimize a Dockerfile for smaller image sizes?\",\n",
    "        \"What is the purpose of the Docker EXPOSE instruction?\",\n",
    "        \"How do you handle environment variables in Docker containers?\",\n",
    "        \n",
    "        # CI/CD - Automation and pipelines (19-23)\n",
    "        \"What are the key stages in a CI/CD pipeline?\",\n",
    "        \"How do you implement automated testing in CI/CD?\",\n",
    "        \"What is the difference between continuous integration and continuous deployment?\",\n",
    "        \"How do you handle secrets and credentials in CI/CD pipelines?\",\n",
    "        \"What are best practices for CI/CD pipeline design?\",\n",
    "        \n",
    "        # DevOps - Practices and methodologies (24-26)\n",
    "        \"What are the core principles of DevOps?\",\n",
    "        \"How does infrastructure as code relate to DevOps?\",\n",
    "        \"What is the role of monitoring and logging in DevOps?\",\n",
    "        \n",
    "        # Google Cloud Platform (27-28)\n",
    "        \"What are the main services offered by Google Cloud Platform?\",\n",
    "        \"How do you deploy applications to Google Cloud Platform?\",\n",
    "        \n",
    "        # Python - Advanced topics (29-30)\n",
    "        \"What are the key modules in the Python standard library?\",\n",
    "        \"How do you use Python's built-in data structures effectively?\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def evaluate_answer_quality(question, answer, context):\n",
    "    \"\"\"\n",
    "    Evaluate answer quality using rule-based metrics (exact copy from utils.py)\n",
    "    \n",
    "    This function provides automated, objective scoring for fair comparison:\n",
    "    - Length: Ensures answers are reasonable length (not too short/long)\n",
    "    - Context Relevance: Measures word overlap with retrieved context\n",
    "    - Question Relevance: Measures word overlap with question\n",
    "    - Not Refusal: Detects if model refuses to answer\n",
    "    \n",
    "    Note: This is rule-based (not semantic), but provides consistent,\n",
    "    objective metrics for comparing baseline vs fine-tuned models.\n",
    "    Both models are evaluated with identical criteria.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    # 1. Length scoring\n",
    "    answer_len = len(answer.strip())\n",
    "    if answer_len == 0:\n",
    "        scores['length_score'] = 0.0\n",
    "    elif answer_len < 20:\n",
    "        scores['length_score'] = 0.5\n",
    "    elif answer_len > 500:\n",
    "        scores['length_score'] = 0.7\n",
    "    else:\n",
    "        scores['length_score'] = 1.0\n",
    "    \n",
    "    # 2. Context relevance\n",
    "    if context:\n",
    "        context_words = set(w.lower() for w in context.split() if len(w) > 3)\n",
    "        answer_words = set(w.lower() for w in answer.split() if len(w) > 3)\n",
    "        if len(context_words) > 0 and len(answer_words) > 0:\n",
    "            overlap = len(context_words & answer_words)\n",
    "            scores['context_relevance'] = min(1.0, overlap / max(len(answer_words), 5))\n",
    "        else:\n",
    "            scores['context_relevance'] = 0.0\n",
    "    else:\n",
    "        scores['context_relevance'] = 0.0\n",
    "    \n",
    "    # 3. Not a refusal\n",
    "    refusal_phrases = [\n",
    "        \"don't know\", \"do not know\", \"cannot answer\", \"can't answer\",\n",
    "        \"no information\", \"not sure\", \"unclear\", \"i'm sorry\",\n",
    "        \"i apologize\", \"unable to\"\n",
    "    ]\n",
    "    answer_lower = answer.lower()\n",
    "    is_refusal = any(phrase in answer_lower for phrase in refusal_phrases)\n",
    "    scores['not_refusal'] = 0.0 if is_refusal else 1.0\n",
    "    \n",
    "    # 4. Question relevance\n",
    "    question_words = set(w.lower() for w in question.split() if len(w) > 3)\n",
    "    answer_words = set(w.lower() for w in answer.split() if len(w) > 3)\n",
    "    if len(question_words) > 0 and len(answer_words) > 0:\n",
    "        overlap = len(question_words & answer_words)\n",
    "        scores['question_relevance'] = min(1.0, overlap / max(len(question_words), 3))\n",
    "    else:\n",
    "        scores['question_relevance'] = 0.0\n",
    "    \n",
    "    # 5. Overall score\n",
    "    metric_values = [\n",
    "        scores['length_score'],\n",
    "        scores['context_relevance'],\n",
    "        scores['not_refusal'],\n",
    "        scores['question_relevance']\n",
    "    ]\n",
    "    scores['overall_score'] = sum(metric_values) / len(metric_values)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "print(\"âœ“ Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¤ Step 1: Upload Your Fine-Tuned Model\n",
    "\n",
    "**How to Create Model ZIP Locally:**\n",
    "```bash\n",
    "# Option 1: Use the export command (recommended)\n",
    "simrag experiment export\n",
    "# Follow the interactive menu to select your model\n",
    "\n",
    "# Option 2: Manual ZIP creation\n",
    "cd tuned_models/model_1b/stage_1/v1.0  # Adjust path to your model\n",
    "# Windows PowerShell:\n",
    "Compress-Archive -Path checkpoint-1000 -DestinationPath checkpoint-1000-fixed.zip\n",
    "# Linux/Mac:\n",
    "zip -r checkpoint-1000-fixed.zip checkpoint-1000/\n",
    "```\n",
    "\n",
    "**What the ZIP Should Contain:**\n",
    "- âœ… `adapter_model.safetensors` (required - LoRA adapter weights)\n",
    "- âœ… `adapter_config.json` (required - model metadata)\n",
    "- âš ï¸ Other files are optional but recommended\n",
    "\n",
    "**Supported Model Types:**\n",
    "- Any checkpoint directory (e.g., `checkpoint-500`, `checkpoint-1000`)\n",
    "- Any version directory (e.g., `v1.8`, `v2.0`)\n",
    "- Stage 1 or Stage 2 models\n",
    "- The notebook will automatically detect model type and metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def find_adapter_dirs(root_path='.'):\n",
    "    \"\"\"\n",
    "    Recursively find directories containing adapter_model.safetensors and adapter_config.json\n",
    "    Handles both flat ZIPs (files at root) and nested ZIPs (files in subdirectories)\n",
    "    \"\"\"\n",
    "    adapter_dirs = []\n",
    "    root = Path(root_path)\n",
    "    \n",
    "    # First, check if files are at the root level (flat ZIP)\n",
    "    adapter_file = root / 'adapter_model.safetensors'\n",
    "    config_file = root / 'adapter_config.json'\n",
    "    if adapter_file.exists() and config_file.exists():\n",
    "        adapter_dirs.append(str(root))\n",
    "        print(f\"  Found at root: {root}\")\n",
    "    \n",
    "    # Then recursively search subdirectories\n",
    "    for path in root.rglob('adapter_model.safetensors'):\n",
    "        dir_path = path.parent\n",
    "        config_path = dir_path / 'adapter_config.json'\n",
    "        if config_path.exists():\n",
    "            rel_path = dir_path.relative_to(root)\n",
    "            if str(rel_path) not in [str(Path(d).relative_to(root)) if Path(d).is_absolute() else d for d in adapter_dirs]:\n",
    "                adapter_dirs.append(str(dir_path))\n",
    "                print(f\"  Found in subdirectory: {rel_path}\")\n",
    "    \n",
    "    return adapter_dirs\n",
    "\n",
    "# Upload checkpoint ZIP\n",
    "print(\"Upload your checkpoint ZIP file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract and find checkpoint files\n",
    "checkpoint_dir = None\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        print(f\"\\nExtracting {filename}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "\n",
    "        # Find adapter_model.safetensors by recursively searching filesystem\n",
    "        print(\"\\nðŸ” Looking for adapter_model.safetensors...\")\n",
    "        \n",
    "        # Recursively search for adapter directories\n",
    "        adapter_dirs = find_adapter_dirs('.')\n",
    "        \n",
    "        if adapter_dirs:\n",
    "            # Use the first found directory (prefer root if both exist)\n",
    "            # Sort to prefer shorter paths (root before nested)\n",
    "            adapter_dirs.sort(key=lambda x: len(str(x)))\n",
    "            checkpoint_dir = adapter_dirs[0]\n",
    "            print(f\"\\nâœ“ Found checkpoint at: {checkpoint_dir}\")\n",
    "\n",
    "            # Verify and show file sizes\n",
    "            checkpoint_path = Path(checkpoint_dir)\n",
    "            required_files = ['adapter_model.safetensors', 'adapter_config.json']\n",
    "            \n",
    "            for req_file in required_files:\n",
    "                file_path = checkpoint_path / req_file\n",
    "                if file_path.exists():\n",
    "                    size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "                    print(f\"  âœ“ {req_file} ({size_mb:.1f} MB)\")\n",
    "                else:\n",
    "                    print(f\"  âœ— {req_file} MISSING!\")\n",
    "            \n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ No adapter files found in ZIP!\")\n",
    "            print(\"\\nExtracted structure:\")\n",
    "            for item in sorted(Path('.').iterdir()):\n",
    "                if item.is_dir() and not item.name.startswith('.'):\n",
    "                    print(f\"  - {item.name}/\")\n",
    "                    # Show first few files in each directory\n",
    "                    try:\n",
    "                        for subitem in list(item.iterdir())[:5]:\n",
    "                            marker = \"/\" if subitem.is_dir() else \"\"\n",
    "                            print(f\"    {subitem.name}{marker}\")\n",
    "                    except:\n",
    "                        pass\n",
    "            raise Exception(\"Required adapter files (adapter_model.safetensors, adapter_config.json) not found in ZIP!\")\n",
    "\n",
    "if not checkpoint_dir:\n",
    "    raise Exception(\"No ZIP file uploaded or no adapter files found!\")\n",
    "\n",
    "# Ensure checkpoint_dir is a string path (not Path object)\n",
    "checkpoint_dir = str(checkpoint_dir)\n",
    "print(f\"\\nâœ… Checkpoint ready: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Model Configuration\n",
    "\n",
    "**Automatic Detection:**\n",
    "The notebook automatically extracts model information from `adapter_config.json`:\n",
    "- Base model name (e.g., `Qwen/Qwen2.5-1.5B-Instruct`)\n",
    "- LoRA rank (r) and alpha parameters\n",
    "- Model size (small/medium)\n",
    "\n",
    "**Auto-Detection:**\n",
    "The notebook attempts to detect stage and version from the checkpoint path. If detection fails, you can manually set:\n",
    "- `MODEL_VERSION`: Your model version (e.g., \"v1.0\")\n",
    "- `STAGE`: Training stage (\"stage_1\" or \"stage_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adapter config to get base model info\n",
    "adapter_path = Path(checkpoint_dir)\n",
    "adapter_config_file = adapter_path / \"adapter_config.json\"\n",
    "\n",
    "# Verify required files exist\n",
    "assert (adapter_path / \"adapter_model.safetensors\").exists(), \"adapter_model.safetensors not found!\"\n",
    "assert adapter_config_file.exists(), \"adapter_config.json not found!\"\n",
    "\n",
    "# Load adapter config\n",
    "with open(adapter_config_file, 'r') as f:\n",
    "    adapter_config = json.load(f)\n",
    "\n",
    "# Extract base model name\n",
    "BASE_MODEL = adapter_config.get(\"base_model_name_or_path\", \"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "print(f\"âœ“ Base model: {BASE_MODEL}\")\n",
    "print(f\"âœ“ LoRA rank (r): {adapter_config.get('r', 'N/A')}\")\n",
    "print(f\"âœ“ LoRA alpha: {adapter_config.get('lora_alpha', 'N/A')}\")\n",
    "\n",
    "# Auto-detect stage and version from path\n",
    "ADAPTER_PATH = checkpoint_dir\n",
    "checkpoint_path = Path(checkpoint_dir)\n",
    "\n",
    "# Try to detect stage from path (look for \"stage_1\" or \"stage_2\")\n",
    "path_str = str(checkpoint_path).lower()\n",
    "if \"stage_2\" in path_str or \"/stage2\" in path_str:\n",
    "    STAGE = \"stage_2\"\n",
    "elif \"stage_1\" in path_str or \"/stage1\" in path_str:\n",
    "    STAGE = \"stage_1\"\n",
    "else:\n",
    "    STAGE = \"stage_1\"  # Default fallback\n",
    "    print(f\"âš ï¸  Could not detect stage from path, defaulting to {STAGE}\")\n",
    "\n",
    "# Try to detect version from path (look for \"v1.0\", \"v2.0\", etc.)\n",
    "version_match = None\n",
    "for part in checkpoint_path.parts:\n",
    "    if part.startswith(\"v\") and len(part) > 1 and part[1:].replace(\".\", \"\").isdigit():\n",
    "        version_match = part\n",
    "        break\n",
    "\n",
    "if version_match:\n",
    "    MODEL_VERSION = version_match\n",
    "else:\n",
    "    # Fallback: try to extract from checkpoint_dir name\n",
    "    dir_name = checkpoint_path.name\n",
    "    if dir_name.startswith(\"v\"):\n",
    "        MODEL_VERSION = dir_name\n",
    "    else:\n",
    "        MODEL_VERSION = \"unknown\"  # User should update manually\n",
    "        print(f\"âš ï¸  Could not detect version from path, set MODEL_VERSION manually\")\n",
    "\n",
    "CHECKPOINT_NAME = checkpoint_path.name\n",
    "MODEL_SIZE = \"small\" if \"1.5B\" in BASE_MODEL or \"1B\" in BASE_MODEL else \"medium\"\n",
    "\n",
    "print(f\"\\nâœ“ Configuration:\")\n",
    "print(f\"  Model size: {MODEL_SIZE}\")\n",
    "print(f\"  Stage: {STAGE}\")\n",
    "print(f\"  Version: {MODEL_VERSION}\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_NAME}\")\n",
    "if MODEL_VERSION == \"unknown\":\n",
    "    print(f\"\\nâš ï¸  Please update MODEL_VERSION manually if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Load Fine-Tuned Model\n",
    "\n",
    "**Memory Optimization:**\n",
    "- Uses 4-bit quantization (BitsAndBytes) to fit in Colab's 15GB VRAM\n",
    "- Loads base model + LoRA adapters via PEFT\n",
    "- Maintains quality while reducing memory usage by ~75%\n",
    "\n",
    "**Expected Load Time:** 2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base model and adapters...\")\n",
    "print(\"This will take 2-3 minutes...\")\n",
    "\n",
    "# 4-bit quantization config (uses ~2GB VRAM for 1.5B model)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load LoRA adapters\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ“ Model loaded successfully!\")\n",
    "print(f\"Model memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 2: Upload Test Documents\n",
    "\n",
    "**Create Documents ZIP Locally:**\n",
    "```bash\n",
    "cd data\n",
    "# Windows PowerShell:\n",
    "Compress-Archive -Path documents -DestinationPath documents.zip\n",
    "# Linux/Mac:\n",
    "zip -r documents.zip documents/\n",
    "```\n",
    "\n",
    "**What Documents Are Used:**\n",
    "- All `.txt`, `.md`, and `.html` files from the documents folder\n",
    "- Same documents used for training and baseline testing\n",
    "- Indexed in ChromaDB for semantic search\n",
    "\n",
    "**Note:** Upload documents **before** running baseline tests. The same document set is used for both baseline and fine-tuned model testing to ensure fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload documents ZIP\n",
    "print(\"Upload your documents ZIP file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract documents\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        print(f\"Extracting {filename}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('./documents')\n",
    "        print(f\"Documents extracted to: ./documents\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Setup RAG System\n",
    "\n",
    "**Critical for Fair Comparison:**\n",
    "This RAG system uses **identical configuration** to your local baseline tests:\n",
    "- **Embedding model**: `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions)\n",
    "  - Same as local: `retriever.py` uses this exact model\n",
    "- **Retrieval**: top_k=5 documents per question\n",
    "  - Matches `RAGConfig.top_k = 5` from your local config\n",
    "- **Prompt format**: `Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:`\n",
    "  - Exact match to `rag_setup.py` lines 147-152\n",
    "- **Database**: In-memory ChromaDB (no persistence)\n",
    "  - Matches `use_persistent=False` in local testing\n",
    "\n",
    "**Why This Matters:**\n",
    "Any differences in results between baseline and fine-tuned models are **only** due to the model's fine-tuning, not configuration differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# RAG Configuration (matches config.py defaults)\n",
    "TOP_K = 5  # Number of documents to retrieve (RAGConfig.top_k = 5)\n",
    "TEMPERATURE = 0.7  # Generation temperature (RAGConfig.temperature = 0.7)\n",
    "MAX_TOKENS = 512  # Max new tokens (generous for complete answers)\n",
    "\n",
    "# Initialize ChromaDB (in-memory, matching your test setup)\n",
    "print(\"Initializing ChromaDB...\")\n",
    "client = chromadb.Client()\n",
    "\n",
    "# CRITICAL: Use exact same embedding model as baseline\n",
    "# This matches retriever.py: SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=\"model_test\",\n",
    "    embedding_function=embedding_fn\n",
    ")\n",
    "\n",
    "print(\"âœ“ Using sentence-transformers/all-MiniLM-L6-v2 (384 dimensions)\")\n",
    "print(f\"âœ“ Retrieval: top_k={TOP_K}\")\n",
    "print(f\"âœ“ Generation: temperature={TEMPERATURE}, max_tokens={MAX_TOKENS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents using the same function as local testing\n",
    "docs_dir = Path(\"./documents\")\n",
    "print(f\"Loading documents from {docs_dir}...\")\n",
    "documents = load_documents_from_folder(str(docs_dir), include_html=True)\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "if not documents:\n",
    "    raise ValueError(\"No documents found!\")\n",
    "\n",
    "# Add to ChromaDB\n",
    "doc_ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    ids=doc_ids\n",
    ")\n",
    "print(\"âœ“ Documents indexed in ChromaDB\")\n",
    "\n",
    "# Store documents and doc_ids for reuse in baseline and fine-tuned tests\n",
    "DOCUMENTS = documents\n",
    "DOC_IDS = doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 3: Run Baseline Tests\n",
    "\n",
    "**Purpose:** Establish baseline performance using the **base model** (no fine-tuning)\n",
    "\n",
    "**What Happens:**\n",
    "1. Loads base model (e.g., `Qwen/Qwen2.5-1.5B-Instruct`) with 4-bit quantization\n",
    "2. Tests all 30 standard questions through the RAG pipeline\n",
    "3. Records: answers, context scores, answer quality, response times\n",
    "4. Cleans up model to free VRAM for fine-tuned model testing\n",
    "\n",
    "**Expected Runtime:** ~10-15 minutes (depends on model size and question complexity)\n",
    "\n",
    "**Note:** Baseline model is loaded **once** and reused for all questions (optimized for speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standard test questions (same as local testing)\n",
    "# These 30 questions are used for both baseline and fine-tuned model testing\n",
    "# to ensure fair comparison\n",
    "TEST_QUESTIONS = get_test_questions()\n",
    "print(f\"Testing with {len(TEST_QUESTIONS)} questions\")\n",
    "print(\"Questions:\", TEST_QUESTIONS)\n",
    "\n",
    "def query_rag_baseline(question, baseline_model, baseline_tokenizer, top_k=TOP_K):\n",
    "    \"\"\"Query RAG system with base model (no fine-tuning)\"\"\"\n",
    "    # Retrieve context\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    context_docs = results['documents'][0] if results['documents'] else []\n",
    "    context_scores = results['distances'][0] if results['distances'] else []\n",
    "    context_scores = [max(0.0, 1.0 - d) for d in context_scores]\n",
    "    \n",
    "    context_text = \"\\n\\n\".join(context_docs)\n",
    "    \n",
    "    # Build prompt - EXACT format from rag_setup.py\n",
    "    prompt = f\"\"\"Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Generate answer using pre-loaded baseline model\n",
    "    inputs = baseline_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(baseline_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = baseline_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=baseline_tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    answer = baseline_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if prompt in answer:\n",
    "        answer = answer.replace(prompt, \"\").strip()\n",
    "    else:\n",
    "        answer_marker = \"Answer:\"\n",
    "        if answer_marker in answer:\n",
    "            answer = answer.split(answer_marker)[-1].strip()\n",
    "    \n",
    "    return answer, context_docs, context_scores\n",
    "\n",
    "def query_rag(question, top_k=TOP_K):\n",
    "    \"\"\"Query RAG system (matches local implementation)\"\"\"\n",
    "    # Retrieve context\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    context_docs = results['documents'][0] if results['documents'] else []\n",
    "    context_scores = results['distances'][0] if results['distances'] else []\n",
    "    # Convert distances to similarity scores (1 - distance for cosine)\n",
    "    # ChromaDB returns L2 distances, but for comparison we normalize to [0,1]\n",
    "    context_scores = [max(0.0, 1.0 - d) for d in context_scores]\n",
    "    \n",
    "    context_text = \"\\n\\n\".join(context_docs)\n",
    "    \n",
    "    # Build prompt - EXACT format from rag_setup.py lines 147-152\n",
    "    prompt = f\"\"\"Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Generate answer with same parameters as baseline\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the answer part (remove prompt)\n",
    "    if prompt in answer:\n",
    "        answer = answer.replace(prompt, \"\").strip()\n",
    "    else:\n",
    "        # Fallback: try to find where answer starts\n",
    "        answer_marker = \"Answer:\"\n",
    "        if answer_marker in answer:\n",
    "            answer = answer.split(answer_marker)[-1].strip()\n",
    "    \n",
    "    return answer, context_docs, context_scores\n",
    "\n",
    "\n",
    "# Run baseline tests\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE TESTS (Base Model)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load baseline model ONCE (optimization - don't reload for each question)\n",
    "print(\"Loading baseline model (this will take 1-2 minutes)...\")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "baseline_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if baseline_tokenizer.pad_token is None:\n",
    "    baseline_tokenizer.pad_token = baseline_tokenizer.eos_token\n",
    "baseline_model.eval()\n",
    "print(\"âœ“ Baseline model loaded\")\n",
    "\n",
    "baseline_results = {\n",
    "    \"experiment_type\": \"baseline\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"model_name\": BASE_MODEL,\n",
    "        \"top_k\": TOP_K,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"documents_folder\": str(docs_dir),\n",
    "        \"num_documents\": len(DOCUMENTS)\n",
    "    },\n",
    "    \"questions\": [],\n",
    "    \"answers\": [],\n",
    "    \"response_times\": [],\n",
    "    \"context_scores\": [],\n",
    "    \"context_docs\": [],\n",
    "    \"answer_quality_scores\": []\n",
    "}\n",
    "\n",
    "print(f\"\\nRunning baseline tests on {len(TEST_QUESTIONS)} questions...\\n\")\n",
    "\n",
    "for i, question in enumerate(TEST_QUESTIONS, 1):\n",
    "    print(f\"[{i}/{len(TEST_QUESTIONS)}] {question[:60]}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        answer, context_docs, context_scores = query_rag_baseline(question, baseline_model, baseline_tokenizer)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        context_text = \"\\n\\n\".join(context_docs)\n",
    "        quality_scores = evaluate_answer_quality(question, answer, context_text)\n",
    "        \n",
    "        baseline_results[\"questions\"].append(question)\n",
    "        baseline_results[\"answers\"].append(answer)\n",
    "        baseline_results[\"response_times\"].append(elapsed)\n",
    "        baseline_results[\"context_scores\"].append(context_scores)\n",
    "        baseline_results[\"context_docs\"].append(context_docs)\n",
    "        baseline_results[\"answer_quality_scores\"].append(quality_scores)\n",
    "        \n",
    "        avg_context = sum(context_scores) / len(context_scores) if context_scores else 0.0\n",
    "        print(f\"  âœ“ Answer: {len(answer)} chars, Context: {avg_context:.3f}, Quality: {quality_scores['overall_score']:.3f}, Time: {elapsed:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        baseline_results[\"questions\"].append(question)\n",
    "        baseline_results[\"answers\"].append(f\"ERROR: {str(e)}\")\n",
    "        baseline_results[\"response_times\"].append(0.0)\n",
    "        baseline_results[\"context_scores\"].append([])\n",
    "        baseline_results[\"context_docs\"].append([])\n",
    "        baseline_results[\"answer_quality_scores\"].append({\n",
    "            \"length_score\": 0.0, \"context_relevance\": 0.0, \"not_refusal\": 0.0,\n",
    "            \"question_relevance\": 0.0, \"overall_score\": 0.0\n",
    "        })\n",
    "\n",
    "# Calculate baseline summary\n",
    "all_baseline_scores = [score for scores in baseline_results[\"context_scores\"] if scores for score in scores]\n",
    "baseline_quality_metrics = {}\n",
    "if baseline_results[\"answer_quality_scores\"]:\n",
    "    metric_keys = [\"length_score\", \"context_relevance\", \"not_refusal\", \"question_relevance\", \"overall_score\"]\n",
    "    for key in metric_keys:\n",
    "        values = [q[key] for q in baseline_results[\"answer_quality_scores\"] if isinstance(q, dict)]\n",
    "        baseline_quality_metrics[f\"avg_{key}\"] = sum(values) / len(values) if values else 0.0\n",
    "\n",
    "baseline_results[\"summary\"] = {\n",
    "    \"avg_context_score\": sum(all_baseline_scores) / len(all_baseline_scores) if all_baseline_scores else 0.0,\n",
    "    \"avg_response_time\": sum(baseline_results[\"response_times\"]) / len(baseline_results[\"response_times\"]) if baseline_results[\"response_times\"] else 0.0,\n",
    "    \"total_questions\": len(TEST_QUESTIONS),\n",
    "    \"successful_queries\": len([a for a in baseline_results[\"answers\"] if not a.startswith(\"ERROR\")]),\n",
    "    **baseline_quality_metrics\n",
    "}\n",
    "\n",
    "print(\"\\n=== Baseline Tests Complete ===\")\n",
    "print(f\"Avg context score: {baseline_results['summary']['avg_context_score']:.3f}\")\n",
    "print(f\"Avg answer quality: {baseline_results['summary'].get('avg_overall_score', 0.0):.3f}\")\n",
    "print(f\"Avg response time: {baseline_results['summary']['avg_response_time']:.2f}s\")\n",
    "\n",
    "# Clean up baseline model to free VRAM for fine-tuned model\n",
    "print(\"\\nCleaning up baseline model...\")\n",
    "del baseline_model\n",
    "del baseline_tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ“ Baseline model unloaded\")\n",
    "\n",
    "# Now run fine-tuned model tests\n",
    "# This uses the SAME test questions, documents, and evaluation metrics\n",
    "# as the baseline to ensure fair comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNED MODEL TESTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Testing fine-tuned model with identical conditions as baseline...\")\n",
    "\n",
    "# Initialize results structure (matches test_model.py format)\n",
    "results = {\n",
    "    \"experiment_type\": \"model_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": {\n",
    "        \"path\": ADAPTER_PATH,\n",
    "        \"stage\": STAGE,\n",
    "        \"version\": MODEL_VERSION,\n",
    "        \"checkpoint\": CHECKPOINT_NAME,\n",
    "        \"provider\": \"colab_transformers\",\n",
    "        \"base_model\": BASE_MODEL,\n",
    "        \"model_size\": MODEL_SIZE,\n",
    "        \"lora_r\": adapter_config.get('r'),\n",
    "        \"lora_alpha\": adapter_config.get('lora_alpha')\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"top_k\": TOP_K,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"documents_folder\": str(docs_dir),\n",
    "        \"num_documents\": len(DOCUMENTS)\n",
    "    },\n",
    "    \"questions\": [],\n",
    "    \"answers\": [],\n",
    "    \"response_times\": [],\n",
    "    \"context_scores\": [],\n",
    "    \"context_docs\": [],\n",
    "    \"answer_quality_scores\": []\n",
    "}\n",
    "\n",
    "print(f\"\\nRunning tests on {len(TEST_QUESTIONS)} questions...\\n\")\n",
    "\n",
    "for i, question in enumerate(TEST_QUESTIONS, 1):\n",
    "    print(f\"[{i}/{len(TEST_QUESTIONS)}] {question[:60]}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        answer, context_docs, context_scores = query_rag(question)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Evaluate answer quality (same as local)\n",
    "        context_text = \"\\n\\n\".join(context_docs)\n",
    "        quality_scores = evaluate_answer_quality(question, answer, context_text)\n",
    "        \n",
    "        results[\"questions\"].append(question)\n",
    "        results[\"answers\"].append(answer)\n",
    "        results[\"response_times\"].append(elapsed)\n",
    "        results[\"context_scores\"].append(context_scores)\n",
    "        results[\"context_docs\"].append(context_docs)\n",
    "        results[\"answer_quality_scores\"].append(quality_scores)\n",
    "        \n",
    "        avg_context = sum(context_scores) / len(context_scores) if context_scores else 0.0\n",
    "        print(f\"  âœ“ Answer: {len(answer)} chars, Context: {avg_context:.3f}, Quality: {quality_scores['overall_score']:.3f}, Time: {elapsed:.1f}s\")\n",
    "        print(f\"    Preview: {answer[:80]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        results[\"questions\"].append(question)\n",
    "        results[\"answers\"].append(f\"ERROR: {str(e)}\")\n",
    "        results[\"response_times\"].append(0.0)\n",
    "        results[\"context_scores\"].append([])\n",
    "        results[\"context_docs\"].append([])\n",
    "        results[\"answer_quality_scores\"].append({\n",
    "            \"length_score\": 0.0,\n",
    "            \"context_relevance\": 0.0,\n",
    "            \"not_refusal\": 0.0,\n",
    "            \"question_relevance\": 0.0,\n",
    "            \"overall_score\": 0.0\n",
    "        })\n",
    "\n",
    "# Calculate summary (matches local format)\n",
    "all_context_scores = [score for scores in results[\"context_scores\"] if scores for score in scores]\n",
    "\n",
    "# Calculate quality metrics\n",
    "quality_metrics = {}\n",
    "if results[\"answer_quality_scores\"]:\n",
    "    metric_keys = [\"length_score\", \"context_relevance\", \"not_refusal\", \"question_relevance\", \"overall_score\"]\n",
    "    for key in metric_keys:\n",
    "        values = [q[key] for q in results[\"answer_quality_scores\"] if isinstance(q, dict)]\n",
    "        quality_metrics[f\"avg_{key}\"] = sum(values) / len(values) if values else 0.0\n",
    "\n",
    "results[\"summary\"] = {\n",
    "    \"avg_context_score\": sum(all_context_scores) / len(all_context_scores) if all_context_scores else 0.0,\n",
    "    \"avg_response_time\": sum(results[\"response_times\"]) / len(results[\"response_times\"]) if results[\"response_times\"] else 0.0,\n",
    "    \"total_questions\": len(TEST_QUESTIONS),\n",
    "    \"successful_queries\": len([a for a in results[\"answers\"] if not a.startswith(\"ERROR\")]),\n",
    "    **quality_metrics\n",
    "}\n",
    "\n",
    "print(\"\\n=== Fine-Tuned Model Tests Complete ===\")\n",
    "print(f\"Avg context score: {results['summary']['avg_context_score']:.3f}\")\n",
    "print(f\"Avg answer quality: {results['summary'].get('avg_overall_score', 0.0):.3f}\")\n",
    "print(f\"Avg response time: {results['summary']['avg_response_time']:.2f}s\")\n",
    "print(f\"Successful: {results['summary']['successful_queries']}/{results['summary']['total_questions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 4: Compare Results\n",
    "\n",
    "**What This Cell Does:**\n",
    "- Calculates raw statistics (mean, std, confidence intervals)\n",
    "- Computes improvement metrics (absolute and percentage changes)\n",
    "- Validates experimental conditions (same questions, documents)\n",
    "- Saves all data to JSON file\n",
    "\n",
    "**Notebook Output:**\n",
    "- âœ… Minimal summary (just confirms it worked)\n",
    "- âœ… Quick stats (baseline vs SimRAG scores)\n",
    "- âœ… Download instructions\n",
    "\n",
    "**Local Display (via `simrag experiment results`):**\n",
    "- ðŸ“Š Formatted tables and comparisons\n",
    "- ðŸ’¡ Interpretations (\"This is a significant improvement\")\n",
    "- ðŸ“ˆ Statistical significance analysis\n",
    "- ðŸŽ¯ Conclusions and recommendations\n",
    "- âœ¨ Rich formatting with colors and structure\n",
    "\n",
    "**Why This Split?**\n",
    "- Notebook focuses on **computation** (raw statistics)\n",
    "- Local display focuses on **interpretation** (what it means)\n",
    "- Easier to read formatted output locally\n",
    "- Can add insights/recommendations without re-running notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs fine-tuned model\n",
    "# This comparison uses the same statistical methods as the local compare_results.py\n",
    "# to ensure consistency across all experiments\n",
    "\n",
    "import math\n",
    "\n",
    "def calculate_stats(scores_list):\n",
    "    \"\"\"\n",
    "    Calculate statistical measures for context scores\n",
    "    \n",
    "    Returns: mean, std, 95% confidence interval\n",
    "    Used for determining statistical significance of improvements\n",
    "    \n",
    "    This matches the statistical analysis in compare_results.py\n",
    "    \"\"\"\n",
    "    all_scores = []\n",
    "    for scores in scores_list:\n",
    "        if isinstance(scores, list):\n",
    "            all_scores.extend(scores)\n",
    "        elif isinstance(scores, (int, float)):\n",
    "            all_scores.append(scores)\n",
    "    \n",
    "    if not all_scores:\n",
    "        return {\"mean\": 0.0, \"std\": 0.0, \"ci_lower\": 0.0, \"ci_upper\": 0.0, \"n\": 0}\n",
    "    \n",
    "    n = len(all_scores)\n",
    "    mean = sum(all_scores) / n\n",
    "    variance = sum((x - mean) ** 2 for x in all_scores) / (n - 1) if n > 1 else 0.0\n",
    "    std = math.sqrt(variance)\n",
    "    se = std / math.sqrt(n) if n > 0 else 0.0\n",
    "    margin = 1.96 * se  # 95% confidence interval\n",
    "    ci_lower = mean - margin\n",
    "    ci_upper = mean + margin\n",
    "    \n",
    "    return {\"mean\": mean, \"std\": std, \"ci_lower\": ci_lower, \"ci_upper\": ci_upper, \"n\": n}\n",
    "\n",
    "baseline_avg = baseline_results[\"summary\"][\"avg_context_score\"]\n",
    "simrag_avg = results[\"summary\"][\"avg_context_score\"]\n",
    "improvement_percent = ((simrag_avg - baseline_avg) / baseline_avg * 100) if baseline_avg > 0 else 0.0\n",
    "\n",
    "baseline_stats = calculate_stats(baseline_results[\"context_scores\"])\n",
    "simrag_stats = calculate_stats(results[\"context_scores\"])\n",
    "\n",
    "baseline_time = baseline_results[\"summary\"][\"avg_response_time\"]\n",
    "simrag_time = results[\"summary\"][\"avg_response_time\"]\n",
    "\n",
    "# Create comparison dictionary\n",
    "# This structure matches compare_results.py for consistency\n",
    "comparison = {\n",
    "    \"comparison_type\": \"baseline_vs_simrag\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"validation\": {\n",
    "        # Validate experimental conditions for fair comparison\n",
    "        \"is_valid\": True,\n",
    "        \"baseline_questions\": len(baseline_results[\"questions\"]),\n",
    "        \"simrag_questions\": len(results[\"questions\"]),\n",
    "        \"same_questions\": baseline_results[\"questions\"] == results[\"questions\"],  # Must be identical\n",
    "        \"baseline_docs\": baseline_results[\"dataset\"][\"num_documents\"],\n",
    "        \"simrag_docs\": results[\"dataset\"][\"num_documents\"],\n",
    "        \"same_docs\": baseline_results[\"dataset\"][\"num_documents\"] == results[\"dataset\"][\"num_documents\"]  # Must be identical\n",
    "    },\n",
    "    \"baseline\": {\n",
    "        \"avg_context_score\": baseline_avg,\n",
    "        \"context_score_stats\": baseline_stats,\n",
    "        \"avg_response_time\": baseline_time,\n",
    "        \"avg_answer_quality\": baseline_results[\"summary\"].get(\"avg_overall_score\", 0.0),\n",
    "        \"num_questions\": len(baseline_results[\"questions\"])\n",
    "    },\n",
    "    \"simrag\": {\n",
    "        \"avg_context_score\": simrag_avg,\n",
    "        \"context_score_stats\": simrag_stats,\n",
    "        \"avg_response_time\": simrag_time,\n",
    "        \"avg_answer_quality\": results[\"summary\"].get(\"avg_overall_score\", 0.0),\n",
    "        \"num_questions\": len(results[\"questions\"]),\n",
    "        \"model\": {\n",
    "            \"stage\": STAGE,\n",
    "            \"version\": MODEL_VERSION,\n",
    "            \"checkpoint\": CHECKPOINT_NAME,\n",
    "            \"base_model\": BASE_MODEL\n",
    "        }\n",
    "    },\n",
    "    \"improvement\": {\n",
    "        \"context_score_improvement\": simrag_avg - baseline_avg,\n",
    "        \"context_score_improvement_percent\": improvement_percent,\n",
    "        \"answer_quality_improvement\": results[\"summary\"].get(\"avg_overall_score\", 0.0) - baseline_results[\"summary\"].get(\"avg_overall_score\", 0.0),\n",
    "        \"response_time_change\": simrag_time - baseline_time,\n",
    "        \"response_time_change_percent\": ((simrag_time - baseline_time) / baseline_time * 100) if baseline_time > 0 else 0.0,\n",
    "        \"statistical_significance\": {\n",
    "            \"baseline_ci\": f\"[{baseline_stats['ci_lower']:.3f}, {baseline_stats['ci_upper']:.3f}]\",\n",
    "            \"simrag_ci\": f\"[{simrag_stats['ci_lower']:.3f}, {simrag_stats['ci_upper']:.3f}]\",\n",
    "            \"overlap\": not (baseline_stats['ci_upper'] < simrag_stats['ci_lower'] or simrag_stats['ci_upper'] < baseline_stats['ci_lower']),\n",
    "            \"note\": \"If CIs don't overlap, difference is statistically significant at p<0.05\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save comparison results (raw statistics only)\n",
    "# All formatting and interpretation will be done locally via \"simrag experiment results\"\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "comparison_file = f\"comparison_results_{timestamp}.json\"\n",
    "\n",
    "with open(comparison_file, 'w') as f:\n",
    "    json.dump(comparison, f, indent=2)\n",
    "\n",
    "# Minimal output - just confirm it worked\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Raw statistics calculated and saved\")\n",
    "print(f\"âœ“ File: {comparison_file}\")\n",
    "print(f\"\\nðŸ“Š Quick Stats:\")\n",
    "print(f\"  Baseline context score: {baseline_avg:.3f}\")\n",
    "print(f\"  SimRAG context score:   {simrag_avg:.3f}\")\n",
    "print(f\"  Improvement:            {improvement_percent:+.1f}%\")\n",
    "print(f\"\\nðŸ“¥ Next Steps:\")\n",
    "print(f\"1. Download: {comparison_file}\")\n",
    "print(f\"2. Place in: comparison_results/ (project root)\")\n",
    "print(f\"3. View formatted results: simrag experiment results\")\n",
    "print(f\"\\nðŸ’¡ Note: Detailed analysis, interpretations, and conclusions\")\n",
    "print(f\"   will be displayed when you run 'simrag experiment results' locally.\")\n",
    "\n",
    "# Download comparison results\n",
    "files.download(comparison_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
